<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>HelloWorld</title>
  
  
  <link href="http://universeinheart.github.io/atom.xml" rel="self"/>
  
  <link href="http://universeinheart.github.io/"/>
  <updated>2022-02-22T00:51:22.815Z</updated>
  <id>http://universeinheart.github.io/</id>
  
  <author>
    <name>xjf</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>09、Redis 切片集群</title>
    <link href="http://universeinheart.github.io/2022/04/16/Redis/09/"/>
    <id>http://universeinheart.github.io/2022/04/16/Redis/09/</id>
    <published>2022-04-16T14:00:00.000Z</published>
    <updated>2022-02-22T00:51:22.815Z</updated>
    
    <content type="html"><![CDATA[<p>在使用 <code>RDB</code> 进行持久化时，Redis 会  <code>fork</code> 子进程来完成，<code>fork</code> 操作的用时和 Redis 的数据量是 <strong>正相关</strong> 的，而 fork 在执行时会阻塞主线程。数据量越大，<code>fork</code>  操作造成的主线程阻塞的时间越长。所以，在使用 RDB 对 25GB 的数据进行持久化时，数据量较大，后台运行的子进程在 fork  创建时阻塞了主线程，于是就导致 Redis 响应变慢了。</p><p>切片集群，也叫分片集群，就是指启动多个  Redis 实例组成一个集群，然后按照一定的规则，把收到的数据划分成多份，每一份用一个实例来保存。 在切片集群中，实例在为 5GB 数据生成 RDB 时，数据量就小了很多，fork 子进程一般不会给主线程带来较长时间的阻塞。采用多个实例保存数据切片后，我们既能保存 25GB 数据，又避免了 fork 子进程阻塞主线程而导致的响应突然变慢。</p><h2 id="数据切片和实例的对应分布关系"><a href="#数据切片和实例的对应分布关系" class="headerlink" title="数据切片和实例的对应分布关系"></a>数据切片和实例的对应分布关系</h2><p>切片集群是一种保存大量数据的通用机制，这个机制可以有不同的实现方案。在 Redis 从3.0 开始，官方提供了一个名为 Redis Cluster 的方案，用于实现切片集群。Redis Cluster 方案中就规定了数据和实例的对应规则。</p><p><code>Redis Cluster</code>  方案采用哈希槽（<code>Hash Slot</code>，接下来我会直接称之为 <code>Slot</code>），来处理数据和实例之间的映射关系。在 <code>Redis Cluster</code>  方案中，一个切片集群共有 <code>16384</code> 个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中。</p><p>首先根据键值对的 key，按照 <code>CRC16</code> 算法计算一个 16 bit 的值；然后，再用这个 16bit 值对 <code>16384</code> 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽</p><p>在部署 <code>Redis Cluster</code> 方案时，可以使用 <code>cluster create</code> 命令创建集群，此时，Redis 会自动把这些槽平均分布在集群实例上。例如，如果集群中有 N 个实例，那么，每个实例上的槽个数为 16384/N 个。我们也可以使用 <code>cluster meet</code> 命令手动建立实例间的连接，形成集群，再使用 <code>cluster addslots</code> 命令，指定每个实例上的哈希槽个数。<strong>在手动分配哈希槽时，需要把 16384 个槽都分配完，否则 Redis 集群无法正常工作。</strong></p><p>假设集群中不同 Redis 实例的内存大小配置不一，如果把哈希槽均分在各个实例上，在保存相同数量的键值对时，和内存大的实例相比，内存小的实例就会有更大的容量压力。遇到这种情况时，你可以根据不同实例的资源配置情况，使用 <code>cluster addslots</code> 命令手动分配哈希槽。</p><h2 id="客户端如何定位数据？"><a href="#客户端如何定位数据？" class="headerlink" title="客户端如何定位数据？"></a>客户端如何定位数据？</h2><blockquote><p>客户端为什么可以在访问任何一个实例时，都能获得所有的哈希槽信息呢？</p><p>这是因为，Redis 实例会把自己的哈希槽信息发给和它相连接的其它实例，来完成哈希槽分配信息的扩散。当实例之间相互连接后，每个实例就有所有哈希槽的映射关系了。</p><p>客户端收到哈希槽信息后，会把哈希槽信息缓存在本地。当客户端请求键值对时，会先计算键所对应的哈希槽，然后就可以给相应的实例发送请求了。</p></blockquote><p>实例和哈希槽的对应关系变更：</p><p>1、在集群中，实例有新增或删除，Redis 需要重新分配哈希槽。</p><p>2、为了负载均衡，Redis 需要把哈希槽在所有实例上重新分布一遍。</p><blockquote><p>客户端是无法主动感知最新的哈希槽分配信息，怎么办？</p><p>答：Redis Cluster 方案提供了一种重定向机制，当客户端把一个键值对的操作请求发给一个实例时，如果这个实例上并没有这个键值对映射的哈希槽，那么，这个实例就会给客户端返回下面的 <code>MOVED</code> 命令响应结果，这个结果中就包含了新实例的访问地址。</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; MOVED 命令表示，客户端请求的键值对所在的哈希槽 13320，实际是在 172.16.19.5 这个实例上</span><br><span class="line">GET hello:key</span><br><span class="line">(error) MOVED 13320 172.16.19.5:6379</span><br></pre></td></tr></table></figure><img src="/2022/04/16/Redis/09/image-20210119151846324.png" class="" title="image-20210119151846324">]]></content>
    
    
    <summary type="html">数据增多了，是该加内存还是加实例？</summary>
    
    
    
    <category term="极客时间Redis实战" scheme="http://universeinheart.github.io/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4Redis%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="Redis" scheme="http://universeinheart.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>10、第1～9讲课后思考题答案及常见问题答疑</title>
    <link href="http://universeinheart.github.io/2022/04/16/Redis/10/"/>
    <id>http://universeinheart.github.io/2022/04/16/Redis/10/</id>
    <published>2022-04-16T14:00:00.000Z</published>
    <updated>2022-02-26T12:07:32.449Z</updated>
    
    <content type="html"><![CDATA[<h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><h3 id="对-SimpleKV-和-Redis-对比"><a href="#对-SimpleKV-和-Redis-对比" class="headerlink" title="对 SimpleKV 和 Redis  对比"></a>对 SimpleKV 和 Redis  对比</h3><img src="/2022/04/16/Redis/10/image-20220222231639875.png" class="" title="image-20220222231639875"><h3 id="整数数组和压缩列表作为底层数据结构的优势是什么？"><a href="#整数数组和压缩列表作为底层数据结构的优势是什么？" class="headerlink" title="整数数组和压缩列表作为底层数据结构的优势是什么？"></a>整数数组和压缩列表作为底层数据结构的优势是什么？</h3><p>整数数组和压缩列表的设计，充分体现了 Redis“又快又省”特点中的“省”，也就是节省内存空间。</p><p>整数数组和压缩列表都是在内存中分配一块地址连续的空间，然后把集合中的元素一个接一个地放在这块空间内，非常紧凑。因为元素是挨个连续放置的，我们不用再通过额外的指针把元素串接起来，这就避免了额外指针带来的空间开销。</p><p>整数数组和压缩列表中的 entry 都是实际的集合元素，它们一个挨一个保存，非常节省内存空间。</p><img src="/2022/04/16/Redis/10/image-20220222231805447.png" class="" title="image-20220222231805447"><h3 id="Redis-基本-IO-模型中还有哪些潜在的性能瓶颈？"><a href="#Redis-基本-IO-模型中还有哪些潜在的性能瓶颈？" class="headerlink" title="Redis 基本 IO 模型中还有哪些潜在的性能瓶颈？"></a>Redis 基本 IO 模型中还有哪些潜在的性能瓶颈？</h3><p>这个问题是希望你能进一步理解阻塞操作对 Redis 单线程性能的影响。在 Redis 基本 IO 模型中，主要是主线程在执行操作，任何耗时的操作，例如 bigkey、全量返回等操作，都是潜在的性能瓶颈。</p><h3 id="AOF-重写过程中有没有其他潜在的阻塞风险？"><a href="#AOF-重写过程中有没有其他潜在的阻塞风险？" class="headerlink" title="AOF 重写过程中有没有其他潜在的阻塞风险？"></a>AOF 重写过程中有没有其他潜在的阻塞风险？</h3><p>风险一：<strong>Redis 主线程 fork 创建 bgrewriteaof 子进程时</strong>，内核需要创建用于管理子进程的相关数据结构，这些数据结构在操作系统中通常叫作进程控制块（Process Control Block，简称为 PCB）。内核要把主线程的 PCB 内容拷贝给子进程。这个创建和拷贝过程由内核执行，是会阻塞主线程的。而且，在拷贝过程中，<strong>子进程要拷贝父进程的页表</strong>，这个过程的耗时和 Redis 实例的内存大小有关。如果 Redis 实例内存大，页表就会大，fork 执行时间就会长，这就会给主线程带来阻塞风险。</p><p>风险二：<strong>bgrewriteaof 子进程会和主线程共享内存</strong>。当主线程收到新写或修改的操作时，主线程会申请新的内存空间，用来保存新写或修改的数据，如果操作的是 bigkey，也就是数据量大的集合类型数据，那么，主线程会因为申请大空间而面临阻塞风险。因为操作系统在分配内存空间时，有查找和锁的开销，这就会导致阻塞。</p><h3 id="AOF-重写为什么不共享使用-AOF-本身的日志？"><a href="#AOF-重写为什么不共享使用-AOF-本身的日志？" class="headerlink" title="AOF 重写为什么不共享使用 AOF 本身的日志？"></a>AOF 重写为什么不共享使用 AOF 本身的日志？</h3><p>如果都用 AOF 日志的话，主线程要写，bgrewriteaof 子进程也要写，这两者会竞争文件系统的锁，这就会对 Redis 主线程的性能造成影响。</p><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>使用一个 2 核 CPU、4GB 内存、500GB 磁盘的云主机运行 Redis，Redis 数据库的数据量大小差不多是 2GB。当时 Redis 主要以修改操作为主，写读比例差不多在 8:2 左右，也就是说，如果有 100 个请求，80 个请求执行的是修改操作。在这个场景下，用 RDB 做持久化有什么风险吗？</p><p><strong>内存不足的风险</strong>：Redis fork 一个 bgsave 子进程进行 RDB 写入，如果主线程再接收到写操作，就会采用写时复制。写时复制需要给写操作的数据分配新的内存空间。本问题中写的比例为 80%，那么，在持久化过程中，为了保存 80% 写操作涉及的数据，写时复制机制会在实例内存中，为这些数据再分配新内存空间，分配的内存量相当于整个实例数据量的 80%，大约是 1.6GB，这样一来，整个系统内存的使用量就接近饱和了。此时，如果实例还有大量的新 key 写入或 key 修改，云主机内存很快就会被吃光。如果云主机开启了 Swap 机制，就会有一部分数据被换到磁盘上，当访问磁盘上的这部分数据时，性能会急剧下降。如果云主机没有开启 Swap，会直接触发 OOM，整个 Redis 实例会面临被系统 kill 掉的风险。</p><p><strong>主线程和子进程竞争使用 CPU 的风险</strong>：生成 RDB 的子进程需要 CPU 核运行，主线程本身也需要 CPU 核运行，而且，如果 Redis 还启用了后台线程，此时，主线程、子进程和后台线程都会竞争 CPU 资源。由于云主机只有 2 核 CPU，这就会影响到主线程处理请求的速度。</p><h3 id="为什么主从库间的复制不使用-AOF？"><a href="#为什么主从库间的复制不使用-AOF？" class="headerlink" title="为什么主从库间的复制不使用 AOF？"></a>为什么主从库间的复制不使用 AOF？</h3><p>RDB 文件是二进制文件，无论是要把 RDB 写入磁盘，还是要通过网络传输 RDB，IO 效率都比记录和传输 AOF 的高。</p><p>在从库端进行恢复时，用 RDB 的恢复效率要高于用 AOF。</p><h3 id="在主从切换过程中，客户端能否正常地进行请求操作呢？"><a href="#在主从切换过程中，客户端能否正常地进行请求操作呢？" class="headerlink" title="在主从切换过程中，客户端能否正常地进行请求操作呢？"></a>在主从切换过程中，客户端能否正常地进行请求操作呢？</h3><p>主从集群一般是采用读写分离模式，当主库故障后，客户端仍然可以把读请求发送给从库，让从库服务。但是，对于写请求操作，客户端就无法执行了。</p><h3 id="如果想要应用程序不感知服务的中断，还需要哨兵或客户端再做些什么吗？"><a href="#如果想要应用程序不感知服务的中断，还需要哨兵或客户端再做些什么吗？" class="headerlink" title="如果想要应用程序不感知服务的中断，还需要哨兵或客户端再做些什么吗？"></a>如果想要应用程序不感知服务的中断，还需要哨兵或客户端再做些什么吗？</h3><p>一方面，客户端需要能缓存应用发送的写请求。只要不是同步写操作（Redis 应用场景一般也没有同步写），写请求通常不会在应用程序的关键路径上，所以，客户端缓存写请求后，给应用程序返回一个确认就行。</p><p>另一方面，主从切换完成后，客户端要能和新主库重新建立连接，哨兵需要提供订阅频道，让客户端能够订阅到新主库的信息。同时，客户端也需要能主动和哨兵通信，询问新主库的信息。</p><h3 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h3><p>5 个哨兵实例的集群，quorum 值设为 2。在运行过程中，如果有 3 个哨兵实例都发生故障了，此时，Redis 主库如果有故障，还能正确地判断主库“客观下线”吗？如果可以的话，还能进行主从库自动切换吗？</p><p>因为判定主库“客观下线”的依据是，认为主库“主观下线”的哨兵个数要大于等于 quorum 值，现在还剩 2 个哨兵实例，个数正好等于 quorum 值，所以还能正常判断主库是否处于“客观下线”状态。如果一个哨兵想要执行主从切换，就要获到半数以上的哨兵投票赞成，也就是至少需要 3 个哨兵投票赞成。但是，现在只有 2 个哨兵了，所以就无法进行主从切换了。</p><h3 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h3><p>哨兵实例是不是越多越好呢？如果同时调大 <code>down-after-milliseconds</code> 值，对减少误判是不是也有好处？</p><p>哨兵实例越多，误判率会越低，但是在判定主库下线和选举 Leader 时，实例需要拿到的赞成票数也越多，等待所有哨兵投完票的时间可能也会相应增加，主从库切换的时间也会变长，客户端容易堆积较多的请求操作，可能会导致客户端请求溢出，从而造成请求丢失。如果业务层对 Redis 的操作有响应时间要求，就可能会因为新主库一直没有选定，新操作无法执行而发生超时报警。</p><p>调大 <code>down-after-milliseconds</code> 后，可能会导致这样的情况：主库实际已经发生故障了，但是哨兵过了很长时间才判断出来，这就会影响到 Redis 对业务的可用性。</p><h3 id="为什么-Redis-不直接用一个表，把键值对和实例的对应关系记录下来？"><a href="#为什么-Redis-不直接用一个表，把键值对和实例的对应关系记录下来？" class="headerlink" title="为什么 Redis 不直接用一个表，把键值对和实例的对应关系记录下来？"></a>为什么 Redis 不直接用一个表，把键值对和实例的对应关系记录下来？</h3><p>如果使用表记录键值对和实例的对应关系，一旦键值对和实例的对应关系发生了变化（例如实例有增减或者数据重新分布），就要修改表。如果是单线程操作表，那么所有操作都要串行执行，性能慢；如果是多线程操作表，就涉及到加锁开销。此外，如果数据量非常大，使用表记录键值对和实例的对应关系，需要的额外存储空间也会增加。</p><p>基于哈希槽计算时，虽然也要记录哈希槽和实例的对应关系，但是哈希槽的个数要比键值对的个数少很多，无论是修改哈希槽和实例的对应关系，还是使用额外空间存储哈希槽和实例的对应关系，都比直接记录键值对和实例的关系的开销小得多。</p><h2 id="典型问题讲解"><a href="#典型问题讲解" class="headerlink" title="典型问题讲解"></a>典型问题讲解</h2><h3 id="rehash-的触发时机和渐进式执行机制"><a href="#rehash-的触发时机和渐进式执行机制" class="headerlink" title="rehash 的触发时机和渐进式执行机制"></a>rehash 的触发时机和渐进式执行机制</h3><h4 id="1-Redis-什么时候做-rehash？"><a href="#1-Redis-什么时候做-rehash？" class="headerlink" title="1.Redis 什么时候做 rehash？"></a>1.Redis 什么时候做 rehash？</h4><p>Redis 会使用装载因子（load factor）来判断是否需要做 rehash。</p><p>装载因子的计算方式是，<strong>哈希表中所有 entry 的个数除以哈希表的哈希桶个数</strong>。</p><p>Redis 会根据装载因子的两种情况，来触发 rehash 操作：</p><ol><li><p>装载因子≥1，同时，哈希表被允许进行 rehash；</p></li><li><p>装载因子≥5。</p></li></ol><p>在第一种情况下，如果装载因子等于 1，同时我们假设，所有键值对是平均分布在哈希表的各个桶中的，那么，此时，哈希表可以不用链式哈希，因为一个哈希桶正好保存了一个键值对。</p><p>但是，如果此时再有新的数据写入，哈希表就要使用链式哈希了，这会对查询性能产生影响。在进行 RDB 生成和 AOF 重写时，哈希表的 rehash 是被禁止的，这是为了避免对 RDB 和 AOF 重写造成影响。如果此时，Redis 没有在生成 RDB 和重写 AOF，那么，就可以进行 rehash。否则的话，再有数据写入时，哈希表就要开始使用查询较慢的链式哈希了。</p><p>在第二种情况下，也就是装载因子大于等于 5 时，就表明当前保存的数据量已经远远大于哈希桶的个数，哈希桶里会有大量的链式哈希存在，性能会受到严重影响，此时，就立马开始做 rehash。</p><p>刚刚说的是触发 rehash 的情况，如果装载因子小于 1，或者装载因子大于 1 但是小于 5，同时哈希表暂时不被允许进行 rehash（例如，实例正在生成 RDB 或者重写 AOF），此时，哈希表是不会进行 rehash 操作的。</p><h4 id="2-采用渐进式-hash-时，如果实例暂时没有收到新请求，是不是就不做-rehash-了？"><a href="#2-采用渐进式-hash-时，如果实例暂时没有收到新请求，是不是就不做-rehash-了？" class="headerlink" title="2.采用渐进式 hash 时，如果实例暂时没有收到新请求，是不是就不做 rehash 了？"></a>2.采用渐进式 hash 时，如果实例暂时没有收到新请求，是不是就不做 rehash 了？</h4><p>其实不是的。Redis 会执行定时任务，定时任务中就包含了 rehash 操作。所谓的定时任务，就是按照一定频率（例如每 100ms/ 次）执行的任务。</p><p>在 rehash 被触发后，即使没有收到新请求，Redis 也会定时执行一次 rehash 操作，而且，每次执行时长不会超过 1ms，以免对其他任务造成影响。</p><h3 id="主线程、子进程和后台线程的联系与区别"><a href="#主线程、子进程和后台线程的联系与区别" class="headerlink" title="主线程、子进程和后台线程的联系与区别"></a>主线程、子进程和后台线程的联系与区别</h3><p>进程和线程的区别。从操作系统的角度来看，进程一般是指资源分配单元，例如一个进程拥有自己的堆、栈、虚存空间（页表）、文件描述符等；而线程一般是指 CPU 进行调度和执行的实体。</p><p>了解了进程和线程的区别后，我们再来看下什么是主进程和主线程。如果一个进程启动后，没有再创建额外的线程，那么，这样的进程一般称为主进程或主线程。 Redis 启动以后，本身就是一个进程，它会接收客户端发送的请求，并处理读写操作请求。而且，接收请求和处理请求操作是 Redis 的主要工作，Redis 没有再依赖于其他线程，所以，我一般把完成这个主要工作的 Redis 进程，称为主进程或主线程。在主线程中，我们还可以使用 fork 创建子进程，或是使用 <code>pthread_create</code> 创建线程。</p><p>下面我先介绍下 Redis 中用 fork 创建的子进程有哪些。</p><ul><li>创建 RDB 的后台子进程，同时由它负责在主从同步时传输 RDB 给从库；</li><li>通过无盘复制方式传输 RDB 的子进程；</li><li>bgrewriteaof 子进程。</li></ul><p>然后，我们再看下 Redis 使用的线程。从 4.0 版本开始，Redis 也开始使用 pthread_create 创建线程，这些线程在创建后，一般会自行执行一些任务，例如执行异步删除任务。相对于完成主要工作的主线程来说，我们一般可以称这些线程为后台线程。关于 Redis 后台线程的具体执行机制，我会在第 16 讲具体介绍。为了帮助你更好地理解，我画了一张图，展示了它们的区别。</p><img src="/2022/04/16/Redis/10/image-20220222232503531.png" class="" title="image-20220222232503531"><h3 id="写时复制的底层实现机制"><a href="#写时复制的底层实现机制" class="headerlink" title="写时复制的底层实现机制"></a>写时复制的底层实现机制</h3><p>Redis 在使用 RDB 方式进行持久化时，会用到写时复制机制。我在第 5 节课讲写时复制的时候，着重介绍了写时复制的效果：<strong>bgsave 子进程相当于复制了原始数据，而主线程仍然可以修改原来的数据</strong>。</p><p>对 Redis 来说，主线程 fork 出 <code>bgsave</code> 子进程后，bgsave 子进程实际是复制了主线程的<strong>页表</strong>。这些页表中，就保存了在执行 bgsave 命令时，主线程的所有数据块在内存中的物理地址。这样一来，bgsave 子进程生成 RDB 时，就可以根据页表读取这些数据，再写入磁盘中。</p><p>如果此时，主线程接收到了新写或修改操作，那么，主线程会使用写时复制机制。具体来说，写时复制就是指，主线程在有写操作时，才会把这个新写或修改后的数据写入到一个新的物理地址中，并修改自己的页表映射。</p><p>我来借助下图中的例子，具体展示一下写时复制的底层机制。bgsave 子进程复制主线程的页表以后，假如主线程需要修改虚页 7 里的数据，那么，主线程就需要新分配一个物理页（假设是物理页 53），然后把修改后的虚页 7 里的数据写到物理页 53 上，而虚页 7 里原来的数据仍然保存在物理页 33 上。这个时候，虚页 7 到物理页 33 的映射关系，仍然保留在 bgsave 子进程中。所以，bgsave 子进程可以无误地把虚页 7 的原始数据写入 RDB 文件。</p><img src="/2022/04/16/Redis/10/image-20220222232524878.png" class="" title="image-20220222232524878"><h3 id="replication-buffer-和-repl-backlog-buffer-的区别"><a href="#replication-buffer-和-repl-backlog-buffer-的区别" class="headerlink" title="replication buffer 和 repl_backlog_buffer 的区别"></a>replication buffer 和 repl_backlog_buffer 的区别</h3><p>在进行主从复制时，Redis 会使用 <code>replication buffer</code> 和 <code>repl_backlog_buffer</code>，有些同学可能不太清楚它们的区别，我再解释下。</p><p>总的来说，<code>replication buffer</code> 是主从库在进行全量复制时，主库上用于和从库连接的客户端的 buffer，而 <code>repl_backlog_buffer</code> 是为了支持从库增量复制，主库上用于持续保存写操作的一块专用 buffer。</p><p>Redis 主从库在进行复制时，当主库要把全量复制期间的写操作命令发给从库时，主库会先创建一个客户端，用来连接从库，然后通过这个客户端，把写操作命令发给从库。在内存中，主库上的客户端就会对应一个 buffer，这个 buffer 就被称为 <code>replication buffer</code>。Redis 通过 client_buffer 配置项来控制这个 buffer 的大小。主库会给每个从库建立一个客户端，所以 replication buffer 不是共享的，而是每个从库都有一个对应的客户端。</p><p><code>repl_backlog_buffer</code> 是一块专用 buffer，在 Redis 服务器启动后，开始一直接收写操作命令，这是所有从库共享的。主库和从库会各自记录自己的复制进度，所以，不同的从库在进行恢复时，会把自己的复制进度（<code>slave_repl_offset</code>）发给主库，主库就可以和它独立同步。</p><img src="/2022/04/16/Redis/10/image-20220222232545874.png" class="" title="image-20220222232545874">]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;思考题&quot;&gt;&lt;a href=&quot;#思考题&quot; class=&quot;headerlink&quot; title=&quot;思考题&quot;&gt;&lt;/a&gt;思考题&lt;/h2&gt;&lt;h3 id=&quot;对-SimpleKV-和-Redis-对比&quot;&gt;&lt;a href=&quot;#对-SimpleKV-和-Redis-对比&quot; class</summary>
      
    
    
    
    <category term="极客时间Redis实战" scheme="http://universeinheart.github.io/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4Redis%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="Redis" scheme="http://universeinheart.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>11、Redis String 为什么不好用了？</title>
    <link href="http://universeinheart.github.io/2022/04/16/Redis/11/"/>
    <id>http://universeinheart.github.io/2022/04/16/Redis/11/</id>
    <published>2022-04-16T14:00:00.000Z</published>
    <updated>2022-02-27T09:14:55.874Z</updated>
    
    <content type="html"><![CDATA[<p>String 类型并不是适用于所有场合的，它有一个明显的短板，就是它 <strong>保存数据时所消耗的内存空间较多</strong></p><p>集合类型有非常节省内存空间的底层实现结构，但是，集合类型保存的数据模式，是一个键对应一系列值，并不适合直接保存单值的键值对。所以，使用 <strong>二级编码</strong>，实现 <strong>用集合类型保存单键值对</strong>，可以明显降低Redis实例的内存空间消耗</p><h3 id="String-类型的内存空间消耗在哪儿了"><a href="#String-类型的内存空间消耗在哪儿了" class="headerlink" title="String 类型的内存空间消耗在哪儿了?"></a>String 类型的内存空间消耗在哪儿了?</h3><p>除了记录实际数据，String 类型还需要额外的内存空间记录数据长度、空间使用等元数据信息</p><blockquote><p>String 类型具体是怎么保存数据的呢？</p><p>保存 64 位有符号整数时，String 类型会把它保存为一个 8 字节的 Long 类型整数，这种保存方式通常也叫作 int 编码方式。</p><p>保存的数据中包含字符时，String 类型就会用简单动态字符串（Simple Dynamic String，SDS）结构体来保存,在 <code>SDS</code> 中，<code>buf</code> 保存实际数据，而 <code>len</code> 和 <code>alloc</code> 本身其实是 <code>SDS</code> 结构体的额外开销。</p><ul><li><code>buf</code>：字节数组，保存实际数据。为了表示字节数组的结束，Redis 会自动在数组最后加一个“\0”，这就会额外占用 1 个字节的开销。</li><li><code>len</code>：占 4 个字节，表示 buf 的已用长度。</li><li><code>alloc</code>：也占个 4  字节，表示 buf 的实际分配长度，一般大于 len。</li></ul></blockquote><p>对于 String 类型来说，除了 SDS 的额外开销，还有一个来自于 RedisObject 结构体的开销。因为 Redis 的数据类型有很多，而且，不同数据类型都有些相同的元数据要记录（比如最后一次访问的时间、被引用的次数等），所以，Redis 会用一个 RedisObject 结构体来统一记录这些元数据，同时指向实际数据。一个 RedisObject 包含了 8 字节的元数据和一个 8 字节指针，这个指针再进一步指向具体数据类型的实际数据所在</p><p> Redis 使用的内存分配库 <code>jemalloc</code> 了。jemalloc 在分配内存时，会根据我们申请的字节数 N，找一个比 N 大，但是最接近 N 的 2 的幂次数作为分配的空间，这样可以减少频繁分配的次数。</p><h3 id="用什么数据结构可以节省内存"><a href="#用什么数据结构可以节省内存" class="headerlink" title="用什么数据结构可以节省内存?"></a>用什么数据结构可以节省内存?</h3><p><strong>压缩列表（ziplist）</strong>，这是一种非常节省内存的结构，压缩列表表头有三个字段 <code>zlbytes</code>、<code>zltail</code> 和 <code>zllen</code>，分别表示 <strong>列表长度</strong>、<strong>列表尾的偏移量</strong>，以及 <strong>列表中的 entry 个数</strong>。压缩列表尾还有一个 <code>zlend</code>，表示列表结束。</p><img src="/2022/04/16/Redis/11/image-20210119201052779.png" class="" title="image-20210119201052779"><p>压缩列表之所以能节省内存，就在于它是用一系列 <strong>连续的 entry 保存数据</strong>。</p><p>每个 entry 的元数据包括下面几部分。</p><ul><li><code>prev_len</code>，表示前一个 entry  的长度。prev_len 有两种取值情况：1 字节或 5 字节。取值 1 字节时，表示上一个 entry 的长度小于 254 字节。虽然 1  字节的值能表示的数值范围是 0 到 255，但是压缩列表中 zlend 的取值默认是 255，因此，就默认用 255  表示整个压缩列表的结束，其他表示长度的地方就不能再用 255 这个值了。所以，当上一个 entry 长度小于 254 字节时，prev_len  取值为 1 字节，否则，就取值为 5 字节。</li><li><code>len</code>：表示自身长度，4 字节；</li><li><code>encoding</code>：表示编码方式，1  字节；</li><li><code>content</code>：保存实际数据。</li></ul><h3 id="如何用集合类型保存单值键值对"><a href="#如何用集合类型保存单值键值对" class="headerlink" title="如何用集合类型保存单值键值对?"></a>如何用集合类型保存单值键值对?</h3><p>采用基于 Hash 类型的二级编码方法。就是把一个单值的数据拆分成两部分，前一部分作为 Hash 集合的 key，后一部分作为 Hash 集合的 value，这样一来，我们就可以把单值数据保存到 Hash 集合中了。</p><p>以图片 ID 1101000060  和图片存储对象 ID 3302000080 为例，我们可以把图片 ID 的前 7 位（1101000）作为 <strong>Hash 类型的键</strong>，把图片 ID  的最后 3 位（060）和图片存储对象 ID 分别作为 <strong>Hash 类型值中的 key 和 value</strong>。按照这种设计方法，我在 Redis  中插入了一组图片 ID 及其存储对象 ID 的记录，并且用 info 命令查看了内存开销，我发现，增加一条记录后，内存占用只增加了 16  字节，如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; info memory</span><br><span class="line"><span class="comment"># Memory</span></span><br><span class="line">used_memory:1039120</span><br><span class="line">127.0.0.1:6379&gt; hset 1101000 060 3302000080</span><br><span class="line">(<span class="built_in">integer</span>) 1</span><br><span class="line">127.0.0.1:6379&gt; info memory</span><br><span class="line"><span class="comment"># Memory</span></span><br><span class="line">used_memory:1039136</span><br></pre></td></tr></table></figure><h3 id="Hash-类型底层结构什么时候使用压缩列表，什么时候使用哈希表呢？"><a href="#Hash-类型底层结构什么时候使用压缩列表，什么时候使用哈希表呢？" class="headerlink" title="Hash 类型底层结构什么时候使用压缩列表，什么时候使用哈希表呢？"></a>Hash 类型底层结构什么时候使用压缩列表，什么时候使用哈希表呢？</h3><p>Hash  类型设置了用压缩列表保存数据时的两个阈值，一旦超过了阈值，Hash  类型就会用哈希表来保存数据了。</p><p>这两个阈值分别对应以下两个配置项：</p><p><code>hash-max-ziplist-entries</code>：表示用压缩列表保存时哈希集合中的 <strong>最大元素个数</strong>。</p><p><code>hash-max-ziplist-value</code>：表示用压缩列表保存时哈希集合中 <strong>单个元素的最大长度</strong>。</p><p>如果我们往 Hash 集合中写入的元素个数超过了 <code>hash-max-ziplist-entries</code>，或者写入的单个元素大小超过了 <code>hash-max-ziplist-value</code>，Redis  就会自动把 Hash 类型的实现结构由 <strong>压缩列表转为哈希表</strong>。一旦从压缩列表转为了哈希表，Hash  类型就会一直用哈希表进行保存，而不会再转回压缩列表了。在节省内存空间方面，哈希表就没有压缩列表那么高效了。</p><p>为了能充分使用压缩列表的精简内存布局，我们一般要 <strong>控制保存在 Hash 集合中的元素个数</strong>。所以，在刚才的二级编码中，我们只用图片 ID 最后 3 位作为 Hash 集合的 key，也就保证了 Hash  集合的元素个数不超过 1000，同时，我们把 hash-max-ziplist-entries 设置为 1000，这样一来，Hash  集合就可以一直使用压缩列表来节省内存空间了。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>这节课，我们打破了对 String 的认知误区，以前，我们认为 String 是“万金油”，什么场合都适用，但是，在保存的键值对本身占用的内存空间不大时（例如这节课里提到的的图片 ID 和图片存储对象 ID），String 类型的元数据开销就占据主导了，这里面包括了 RedisObject 结构、SDS 结构、dictEntry 结构的内存开销。</p><p>针对这种情况，我们可以使用压缩列表保存数据。当然，使用 Hash 这种集合类型保存单值键值对的数据时，我们需要将单值数据拆分成两部分，分别作为 Hash 集合的键和值，就像刚才案例中用二级编码来表示图片 ID，希望你能把这个方法用到自己的场景中。</p><p>最后，我还想再给你提供一个小方法：如果你想知道键值对采用不同类型保存时的内存开销，可以在这个网址里<a href="http://www.redis.cn/redis_memory/%E8%BE%93%E5%85%A5%E4%BD%A0%E7%9A%84%E9%94%AE%E5%80%BC%E5%AF%B9%E9%95%BF%E5%BA%A6%E5%92%8C%E4%BD%BF%E7%94%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%EF%BC%8C%E8%BF%99%E6%A0%B7%E5%B0%B1%E8%83%BD%E7%9F%A5%E9%81%93%E5%AE%9E%E9%99%85%E6%B6%88%E8%80%97%E7%9A%84%E5%86%85%E5%AD%98%E5%A4%A7%E5%B0%8F%E4%BA%86%E3%80%82%E5%BB%BA%E8%AE%AE%E4%BD%A0%E6%8A%8A%E8%BF%99%E4%B8%AA%E5%B0%8F%E5%B7%A5%E5%85%B7%E7%94%A8%E8%B5%B7%E6%9D%A5%EF%BC%8C%E5%AE%83%E5%8F%AF%E4%BB%A5%E5%B8%AE%E5%8A%A9%E4%BD%A0%E5%85%85%E5%88%86%E5%9C%B0%E8%8A%82%E7%9C%81%E5%86%85%E5%AD%98%E3%80%82">http://www.redis.cn/redis_memory/输入你的键值对长度和使用的数据类型，这样就能知道实际消耗的内存大小了。建议你把这个小工具用起来，它可以帮助你充分地节省内存。</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;String 类型并不是适用于所有场合的，它有一个明显的短板，就是它 &lt;strong&gt;保存数据时所消耗的内存空间较多&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;集合类型有非常节省内存空间的底层实现结构，但是，集合类型保存的数据模式，是一个键对应一系列值，并不适合直接保存单值的键值对。</summary>
      
    
    
    
    <category term="极客时间Redis实战" scheme="http://universeinheart.github.io/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4Redis%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="Redis" scheme="http://universeinheart.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>04、Redis AOF日志</title>
    <link href="http://universeinheart.github.io/2022/04/10/Redis/04/"/>
    <id>http://universeinheart.github.io/2022/04/10/Redis/04/</id>
    <published>2022-04-10T11:50:15.000Z</published>
    <updated>2022-02-13T03:32:52.835Z</updated>
    
    <content type="html"><![CDATA[<p>Redis 的持久化主要有两大机制，即 **AOF（Append Only File）日志 **和  <strong>RDB 快照</strong></p><h2 id="AOF-日志是如何实现的？"><a href="#AOF-日志是如何实现的？" class="headerlink" title="AOF 日志是如何实现的？"></a>AOF 日志是如何实现的？</h2><p>数据库的 <strong>写前日志（Write Ahead Log, WAL）</strong>，在实际写数据前，先把修改的数据记到日志文件中，以便故障时进行恢复。</p><p>不过，<code>AOF</code> 日志正好相反，它是 <strong>写后日志</strong>，“写后”的意思是 <strong>Redis 是先执行命令，把数据写入内存，然后才记录日志</strong>，如下图所示：</p><img src="/2022/04/10/Redis/04/image-20210111232040747.png" class="" title="image-20210111232040747"><h3 id="那-AOF-为什么要先执行命令再记日志呢？"><a href="#那-AOF-为什么要先执行命令再记日志呢？" class="headerlink" title="那 AOF 为什么要先执行命令再记日志呢？"></a>那 AOF 为什么要先执行命令再记日志呢？</h3><p>传统数据库的日志，例如 <strong>redo log（重做日志）</strong>，记录的是修改后的数据，而 AOF 里记录的是 <strong>Redis 收到的每一条命令</strong>，这些命令是以文本形式保存的。</p><blockquote><p>以 Redis 收到“set testkey testvalue”命令后记录的日志为例，看看 AOF 日志的内容。</p><p>其中，“*3”表示当前命令有三个部分，每部分都是由“$+数字”开头，后面紧跟着具体的命令、键或值。这里，“数字”表示这部分中的命令、键或值一共有多少字节。例如，“$3 set”表示这部分有 3 个字节，也就是“set”命令。</p></blockquote><img src="/2022/04/10/Redis/04/image-20210111232612880.png" class="" title="image-20210111232612880"><p>为了避免额外的检查开销，Redis 在向 AOF 里面记录日志的时候，并不会先去对这些命令进行语法检查。所以，如果先记日志再执行命令的话，日志中就有可能记录了错误的命令，Redis 在使用日志恢复数据时，就可能会出错。而写后日志这种方式，就是先让系统执行命令，只有命令能执行成功，才会被记录到日志中，否则，系统就会直接向客户端报错。</p><p>所以，Redis 使用写后日志这一方式的一大好处是，可以 <strong>避免出现记录错误命令</strong> 的情况。AOF 还有一个好处：它是在命令执行后才记录日志，所以<strong>不会阻塞当前的写操作</strong>。</p><h3 id="AOF-也有两个潜在的风险"><a href="#AOF-也有两个潜在的风险" class="headerlink" title="AOF 也有两个潜在的风险"></a>AOF 也有两个潜在的风险</h3><p>首先，如果刚执行完一个命令，还没有来得及记日志就宕机了，那么这个命令和相应的数据就有丢失的风险。如果此时 Redis 是用作缓存，还可以从后端数据库重新读入数据进行恢复，但是，如果 Redis 是直接用作数据库的话，此时，因为命令没有记入日志，所以就无法用日志进行恢复了。</p><p>其次，AOF 虽然避免了对当前命令的阻塞，但可能会给下一个操作带来阻塞风险。这是因为，AOF 日志也是在主线程中执行的，如果在把日志文件写入磁盘时，磁盘写压力大，就会导致写盘很慢，进而导致后续的操作也无法执行了。</p><p>仔细分析的话，你就会发现，这两个风险都是和 <strong>AOF 写回磁盘的时机</strong>相关的。这也就意味着，如果我们能够控制一个写命令执行完后 AOF 日志写回磁盘的时机，这两个风险就解除了。</p><h2 id="三种写回策略"><a href="#三种写回策略" class="headerlink" title="三种写回策略"></a>三种写回策略</h2><p>AOF 机制给我们提供了三个选择，也就是 AOF 配置项 <code>appendfsync</code> 的三个可选值。</p><ul><li><strong>Always</strong>，<strong>同步写回</strong>：每个写命令执行完，立马同步地将日志写回磁盘；不可避免地会影响主线程性能</li><li><strong>Everysec</strong>，<strong>每秒写回</strong>：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘；在避免影响主线程性能和避免数据丢失两者间取了个折中。</li><li><strong>No</strong>，<strong>操作系统控制的写回</strong>：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘。落盘的时机已经不在 Redis 手中了，只要 AOF 记录没有写回磁盘，一旦宕机对应的数据就丢失了</li></ul><img src="/2022/04/10/Redis/04/image-20210111233115916.png" class="" title="image-20210111233115916"><p>随着接收的写命令越来越多，AOF 文件会越来越大。一定要小心 AOF 文件过大带来的性能问题。</p><p>一、文件系统本身对文件大小有限制，无法保存过大的文件</p><p>二、如果文件太大，之后再往里面追加命令记录的话，效率也会变低</p><p>三、如果发生宕机，AOF 中记录的命令要一个个被重新执行，用于故障恢复，如果日志文件太大，整个恢复过程就会非常缓慢，这就会影响到 Redis 的正常使用。</p><h2 id="日志文件太大了怎么办？（AOF-重写机制）"><a href="#日志文件太大了怎么办？（AOF-重写机制）" class="headerlink" title="日志文件太大了怎么办？（AOF 重写机制）"></a>日志文件太大了怎么办？（AOF 重写机制）</h2><p>重写机制具有“多变一”功能。旧日志文件中的多条命令，在重写后的新日志中变成了一条命令。</p><p><code>AOF</code> 文件是以追加的方式，逐一记录接收到的写命令的。当一个键值对被多条写命令反复修改时，AOF 文件会记录相应的多条命令。但是，在重写的时候，是根据这个键值对当前的最新状态，为它生成对应的写入命令。这样一来，一个键值对在重写日志中只用一条命令就行了，而且，在日志恢复时，只用执行这条命令，就可以直接完成这个键值对的写入了。</p><img src="/2022/04/10/Redis/04/image-20210113224725050.png" class="" title="image-20210113224725050"><p>虽然 AOF 重写后，日志文件会缩小，但是，要把整个数据库的最新数据的操作日志都写回磁盘，仍然是一个非常耗时的过程。这时，我们就要继续关注另一个问题了：重写会不会阻塞主线程？</p><h2 id="AOF-重写会阻塞吗"><a href="#AOF-重写会阻塞吗" class="headerlink" title="AOF 重写会阻塞吗?"></a>AOF 重写会阻塞吗?</h2><p>和 <strong>AOF 日志由主线程写回</strong> 不同，重写过程是由<strong>后台子进程</strong> <code>bgrewriteaof</code> 来完成的，这也是为了避免阻塞主线程，导致数据库性能下降。</p><p>每次执行重写时，主线程 <code>fork</code> 出后台的 <code>bgrewriteaof</code> 子进程。此时，fork 会把主线程的内存拷贝一份给 <code>bgrewriteaof</code> 子进程，这里面就包含了数据库的最新数据。然后，<code>bgrewriteaof</code> 子进程就可以在不影响主线程的情况下，逐一把拷贝的数据写成操作，记入重写日志。</p><p>因为主线程未阻塞，仍然可以处理新来的操作。此时，如果有写操作，Redis 会把这个操作写到它的缓冲区。这样一来，即使宕机了，这个 AOF 日志的操作仍然是齐全的，可以用于恢复。这个操作也会被写到重写日志的缓冲区。这样，重写日志也不会丢失最新的操作。等到拷贝数据的所有操作记录重写完成后，重写日志记录的这些最新操作也会写入新的 AOF 文件，以保证数据库最新状态的记录。此时，我们就可以用新的 AOF 文件替代旧文件了。</p><img src="/2022/04/10/Redis/04/image-20210113233333814.png" class="" title="image-20210113233333814"><p>每次 AOF 重写时，Redis 会先执行一个内存拷贝，用于重写；然后，使用两个日志保证在重写过程中，新写入的数据不会丢失。而且，因为 Redis 采用额外的线程进行数据重写，所以，这个过程并不会阻塞主线程。</p><h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h3><p>AOF 重写也有一个重写日志，为什么它不共享使用 AOF 本身的日志呢？</p><p>答：AOF重写不复用AOF本身的日志，一个原因是父子进程写同一个文件必然会产生竞争问题，控制竞争就意味着会影响父进程的性能。二是如果AOF重写过程中失败了，那么原本的AOF文件相当于被污染了，无法做恢复使用。所以Redis AOF重写一个新文件，重写失败的话，直接删除这个文件就好了，不会对原先的AOF文件产生影响。等重写完成之后，直接替换旧文件即可。</p>]]></content>
    
    
    <summary type="html">宕机了，Redis如何避免数据丢失</summary>
    
    
    
    <category term="极客时间Redis实战" scheme="http://universeinheart.github.io/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4Redis%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="Redis" scheme="http://universeinheart.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>可观测性</title>
    <link href="http://universeinheart.github.io/2022/04/09/%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84-%E6%9E%84%E5%BB%BA%E5%8F%AF%E9%9D%A0%E7%9A%84%E5%A4%A7%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/10/"/>
    <id>http://universeinheart.github.io/2022/04/09/%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84-%E6%9E%84%E5%BB%BA%E5%8F%AF%E9%9D%A0%E7%9A%84%E5%A4%A7%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/10/</id>
    <published>2022-04-09T15:26:37.000Z</published>
    <updated>2022-04-09T08:57:46.299Z</updated>
    
    <content type="html"><![CDATA[<p>学术界一般会将可观测性分解为三个更具体方向进行研究，分别是：<strong>事件日志</strong>、<strong>链路追踪</strong>和<strong>聚合度量</strong></p><h2 id="事件日志"><a href="#事件日志" class="headerlink" title="事件日志"></a>事件日志</h2><p><strong>日志（Logging）</strong>：日志的职责是记录离散事件，通过这些记录事后分析出程序的行为，譬如曾经调用过什么方法，曾经操作过哪些数据，等等。打印日志被认为是程序中最简单的工作之一，调试问题时常有人会说“当初这里记得打点日志就好了”，可见这就是一项举手之劳的任务。输出日志的确很容易，但收集和分析日志却可能会很复杂，面对成千上万的集群节点，面对迅速滚动的事件信息，面对数以 TB 计算的文本，传输与归集都并不简单。 </p><h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><p><strong>日志输出应该避免的</strong></p><ul><li><strong>避免打印敏感信息</strong>。 </li><li><strong>避免引用慢操作</strong>。日志中打印的信息应该是上下文中可以直接取到的，如果当前上下文中根本没有这项数据，需要专门调用远程服务或者从数据库获取，又或者通过大量计算才能取到的话，那应该先考虑这项信息放到日志中是不是必要且恰当的。</li><li><strong>避免打印追踪诊断信息</strong>。日志中不要打印方法输入参数、输出结果、方法执行时长之类的调试信息。这个观点是反直觉的，不少公司甚至会将其作为最佳实践来提倡，但是笔者仍坚持将其归入反模式中。日志的职责是记录事件，追踪诊断应由追踪系统去处理，哪怕贵公司完全没有开发追踪诊断方面功能的打算，笔者也建议使用<a href="https://github.com/btraceio/btrace">BTrace</a>或者<a href="https://github.com/alibaba/arthas">Arthas</a>这类“On-The-Fly”的工具来解决。之所以将其归为反模式，是因为上面说的<strong>敏感信息</strong>、<strong>慢操作</strong>等的主要源头就是这些原本想用于调试的日志。譬如，当前方法的返回值是个 Map，开发期的调试数据只做了三五个 Entity，觉得遍历一下把具体内容打到日志里面没什么问题，到了生产期，这个 Map 里面有可能存放了成千上万个 Entity，这时候打印日志就相当于引用慢操作。</li><li><strong>避免误导他人</strong>。譬如明明已经在逻辑中妥善处理好了某个异常，偏习惯性地调用 printStackTrace()方法，把堆栈打到日志中，一旦这个方法附近出现问题，由其他人来除错的话，很容易会盯着这段堆栈去找线索而浪费大量时间。</li></ul><p><strong>日志输出必不可少的</strong></p><ul><li><strong>处理请求时的 TraceID</strong>。服务收到请求时，如果该请求没有附带 TraceID，就应该自动生成唯一的 TraceID 来对请求进行标记，并使用 MDC 自动输出到日志。TraceID 会贯穿整条调用链，目的是通过它把请求在分布式系统各个服务中的执行过程串联起来。TraceID 通常也会随着请求的响应返回到客户端，如果响应内容出现了异常，用户便能通过此 ID 快速找到与问题相关的日志。TraceID 是链路追踪里的概念，类似的还有用于标识进程内调用状况的 SpanID，在 Java 程序中这些都可以用 Spring Cloud Sleuth 来自动生成。尽管 TraceID 在分布式跟踪会发挥最大的作用，但即使对单体系统，将 TraceID 记录到日志并返回给最终用户，对快速定位错误仍然十分有价值。</li><li><strong>系统运行过程中的关键事件</strong>。日志的职责就是记录事件，进行了哪些操作、发生了与预期不符的情况、运行期间出现未能处理的异常或警告、定期自动执行的任务，等等，都应该在日志中完整记录下来。原则上程序中发生的事件只要有价值就应该去记录，但应判断清楚事件的重要程度，选定相匹配的日志的级别。至于如何快速处理大量日志，这是后面步骤要考虑的问题，如果输出日志实在太频繁以至于影响性能，应由运维人员去调整全局或单个类的日志级别来解决。</li><li><strong>启动时输出配置信息</strong>。与避免输出诊断信息不同，对于系统启动时或者检测到配置中心变化时更新的配置，应将非敏感的配置信息输出到日志中，譬如连接的数据库、临时目录的路径等等，初始化配置的逻辑一般只会执行一次，不便于诊断时复现，所以应该输出到日志中。</li></ul><h3 id="收集与缓冲"><a href="#收集与缓冲" class="headerlink" title="收集与缓冲"></a>收集与缓冲</h3><p>分布式系统处理一个请求要跨越多个服务节点，为了能看到跨节点的全部日志，就要有能覆盖整个链路的全局日志系统。这个需求决定了每个节点输出日志到文件后，必须将日志文件统一收集起来集中存储、索引，由此便催生了专门的日志收集器。</p><p>最初，ELK 中日志收集与下一节要讲的加工聚合的职责都是由 <code>Logstash</code> 来承担的，<code>Logstash</code> 除了部署在各个节点中作为收集的客户端（Shipper）以外，它还同时设有独立部署的节点，扮演归集转换日志的服务端（Master）角色。Logstash 有良好的插件化设计，收集、转换、输出都支持插件化定制，应对多重角色本身并没有什么困难。但是 Logstash 与它的插件是基于 JRuby 编写的，要跑在单独的 Java 虚拟机进程上，而且 Logstash 的默认的堆大小就到了 1GB。对于归集部分（Master）这种消耗并不是什么问题，但作为每个节点都要部署的日志收集器就显得太过负重了。后来，Elastic.co 公司将所有需要在服务节点中处理的工作整理成以Libbeat为核心的Beats 框架，并使用 Golang 重写了一个功能较少，却更轻量高效的日志收集器，这就是今天流行的<code>Filebeat</code>。</p><p>日志收集器不仅要保证能覆盖全部数据来源，还要尽力保证日志数据的连续性，这其实并不容易做到。收集到系统中的日志要与实际产生的日志保持绝对的一致性是非常困难的，也不应该为此付出过高成本。换而言之，日志不追求绝对的完整精确，只追求在代价可承受的范围内保证尽可能地保证较高的数据质量。一种最常用的缓解压力的做法是将日志接收者从 <code>Logstash</code> 和 <code>Elasticsearch</code> 转移至抗压能力更强的队列缓存，譬如在 <code>Logstash</code> 之前架设一个 <code>Kafka</code> 或者 <code>Redis</code> 作为缓冲层，面对突发流量，<code>Logstash</code> 或 <code>Elasticsearch</code> 处理能力出现瓶颈时自动削峰填谷，甚至当它们短时间停顿，也不会丢失日志数据。</p><h3 id="加工与聚合"><a href="#加工与聚合" class="headerlink" title="加工与聚合"></a>加工与聚合</h3><p>将日志集中收集之后，存入 <code>Elasticsearch</code> 之前，一般还要对它们进行加工转换和聚合处理。这是因为日志是非结构化数据，一行日志中通常会包含多项信息，如果不做处理，那在 <code>Elasticsearch</code> 就只能以全文检索的原始方式去使用日志，既不利于统计对比，也不利于条件过滤。</p><p><code>Logstash</code> 的基本职能是把日志行中的非结构化数据，通过 Grok 表达式语法转换为上面表格那样的结构化数据，进行结构化的同时，还可能会根据需要，调用其他插件来完成时间处理（统一时间格式）、类型转换（如字符串、数值的转换）、查询归类（譬如将 IP 地址根据地理信息库按省市归类）等额外处理工作，然后以指定格式输出到 Elasticsearch 中。有了这些经过 <code>Logstash</code> 转换，已经结构化的日志，<code>Elasticsearch</code> 便可针对不同的数据项来建立索引，进行条件查询、统计、聚合等操作的了。</p><h3 id="存储与查询"><a href="#存储与查询" class="headerlink" title="存储与查询"></a>存储与查询</h3><p>经过收集、缓冲、加工、聚合的日志数据，终于可以放入 <code>Elasticsearch</code> 中索引存储了。<code>Elasticsearch</code> 是整个 <code>Elastic Stack</code> 技术栈的核心，其他步骤的工具，如 <code>Filebeat</code>、<code>Logstash</code>、<code>Kibana</code> 都有替代品，有自由选择的余地，唯独 <code>Elasticsearch</code> 在日志分析这方面完全没有什么值得一提的竞争者。这样的结果肯定与 <code>Elasticsearch</code> 本身是一款优秀产品有关，然而更关键的是 <code>Elasticsearch</code> 的优势正好与日志分析的需求完美契合：</p><ul><li>从数据特征的角度看，日志是典型的基于时间的数据流，日志的数据特征决定了所有用于日志分析的 Elasticsearch 都会使用时间范围作为索引，根据实际数据量的大小可能是按月、按周或者按日、按时。 </li><li>从数据价值的角度看，日志基本上只会以最近的数据为检索目标，随着时间推移，早期的数据将逐渐失去价值。这点决定了可以很容易区分出冷数据和热数据，进而对不同数据采用不一样的硬件策略。譬如为热数据配备 SSD 磁盘和更好的处理器，为冷数据配备 HDD 磁盘和较弱的处理器，甚至可以放到更为廉价的对象存储（如阿里云的 OSS）中归档。</li><li>从数据使用的角度看，分析日志对实时性的要求是处于实时与离线两者之间的“近实时”，即不强求日志产生后立刻能查到，但也不能接受日志产生之后按小时甚至按天的频率来更新，这些检索能力和近实时性，也正好都是 Elasticsearch 的强项。</li></ul><p>Elasticsearch 只提供了 API 层面的查询能力，它通常搭配同样出自 Elastic.co 公司的 Kibana 一起使用，可以将 Kibana 视为 Elastic Stack 的 GUI 部分。Kibana 尽管只负责图形界面和展示，但它提供的能力远不止让你能在界面上执行 Elasticsearch 的查询那么简单。Kibana 宣传的核心能力是“探索数据并可视化”，即把存储在 Elasticsearch 中的数据被检索、聚合、统计后，定制形成各种图形、表格、指标、统计，以此观察系统的运行状态，找出日志事件中潜藏的规律和隐患。</p><h2 id="链路追踪"><a href="#链路追踪" class="headerlink" title="链路追踪"></a>链路追踪</h2><p><strong>追踪（Tracing）</strong>：单体系统时代追踪的范畴基本只局限于栈追踪（<code>Stack Tracing</code>），调试程序时，在 IDE 打个断点，看到的 Call Stack 视图上的内容便是追踪；编写代码时，处理异常调用了 <code>Exception::printStackTrace()</code>方法，它输出的堆栈信息也是追踪。微服务时代，追踪就不只局限于调用栈了，一个外部请求需要内部若干服务的联动响应，这时候完整的调用轨迹将跨越多个服务，同时包括服务间的网络传输信息与各个服务内部的调用堆栈信息，因此，分布式系统中的追踪在国内常被称为“全链路追踪”。追踪的主要目的是排查故障，如分析调用链的哪一部分、哪个方法出现错误或阻塞，输入输出是否符合预期，等等。</p><h3 id="追踪与跨度"><a href="#追踪与跨度" class="headerlink" title="追踪与跨度"></a>追踪与跨度</h3><p>从客户端发起请求抵达系统的边界开始，记录请求流经的每一个服务，直到到向客户端返回响应为止，这整个过程就称为一次“追踪”（Trace）。由于每次 Trace 都可能会调用数量不定、坐标不定的多个服务，为了能够记录具体调用了哪些服务，以及调用的顺序、开始时点、执行时长等信息，每次开始调用服务前都要先埋入一个调用记录，这个记录称为一个“跨度”（Span）。Span 的数据结构应该足够简单，以便于能放在日志或者网络协议的报文头里；也应该足够完备，起码应含有时间戳、起止时间、Trace 的 ID、当前 Span 的 ID、父 Span 的 ID 等能够满足追踪需要的信息。</p><p>每一次 Trace 实际上都是由若干个有顺序、有层级关系的 Span 所组成一颗“追踪树”（Trace Tree），如图所示</p><img src="/2022/04/09/%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84-%E6%9E%84%E5%BB%BA%E5%8F%AF%E9%9D%A0%E7%9A%84%E5%A4%A7%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/10/image-20220409164605441.png" class="" title="image-20220409164605441"><h3 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h3><p>目前，追踪系统根据数据收集方式的差异，可分为三种主流的实现方式，分别是<strong>基于日志的追踪</strong>（Log-Based Tracing），<strong>基于服务的追踪</strong>（Service-Based Tracing）和<strong>基于边车代理的追踪</strong>（Sidecar-Based Tracing），笔者分别介绍如下：</p><ul><li><strong>基于日志的追踪</strong>的思路是将 Trace、Span 等信息直接输出到应用日志中，然后随着所有节点的日志归集过程汇聚到一起，再从全局日志信息中反推出完整的调用链拓扑关系。日志追踪对网络消息完全没有侵入性，对应用程序只有很少量的侵入性，对性能影响也非常低。但其缺点是直接依赖于日志归集过程，日志本身不追求绝对的连续与一致，这也使得基于日志的追踪往往不如其他两种追踪实现来的精准。</li><li><strong>基于服务的追踪</strong>是目前最为常见的追踪实现方式，被 Zipkin、SkyWalking、Pinpoint 等主流追踪系统广泛采用。服务追踪的实现思路是通过某些手段给目标应用注入追踪探针（Probe），针对 Java 应用一般就是通过 Java Agent 注入的。探针在结构上可视为一个寄生在目标服务身上的小型微服务系统，它一般会有自己专用的服务注册、心跳检测等功能，有专门的数据收集协议，把从目标系统中监控得到的服务调用信息，通过另一次独立的 HTTP 或者 RPC 请求发送给追踪系统。</li><li><strong>基于边车代理的追踪</strong>是服务网格的专属方案，也是最理想的分布式追踪模型，它对应用完全透明，无论是日志还是服务本身都不会有任何变化；它与程序语言无关，无论应用采用什么编程语言实现，只要它还是通过网络（HTTP 或者 gRPC）来访问服务就可以被追踪到；它有自己独立的数据通道，追踪数据通过控制平面进行上报，避免了追踪对程序通信或者日志归集的依赖和干扰，保证了最佳的精确性。</li></ul><h3 id="追踪规范化"><a href="#追踪规范化" class="headerlink" title="追踪规范化"></a>追踪规范化</h3><p><code>OpenTelemetry</code> 一诞生就带着无比炫目的光环，直接进入 CNCF 的孵化项目，它的目标是统一追踪、度量和日志三大领域（目前主要关注的是追踪和度量，日志方面，官方表示将放到下一阶段再去处理）。不过，OpenTelemetry 毕竟是 2019 年才出现的新生事物，尽管背景渊源深厚，前途光明，但未来究竟如何发展，能否打败现在已经有的众多成熟系统，目前仍然言之尚早。</p><h2 id="聚合度量"><a href="#聚合度量" class="headerlink" title="聚合度量"></a>聚合度量</h2><p><strong>度量（Metrics）</strong>：度量是指对系统中某一类信息的统计聚合。目的是揭示系统的总体运行状态。Java 天生自带有一种基本的度量，就是由虚拟机直接提供的 JMX（Java Management eXtensions）度量，诸如内存大小、各分代的用量、峰值的线程数、垃圾收集的吞吐量、频率，等等都可以从 JMX 中获得。度量的主要目的是监控（Monitoring）和预警（Alert），如某些度量指标达到风险阈值时触发事件，以便自动处理或者提醒管理员介入。</p><p> <a href="https://prometheus.io/">Prometheus</a>在度量领域的统治力虽然还暂时不如日志领域中 Elastic Stack 的统治地位那么稳固，但在云原生时代里，基本也已经能算是事实标准了</p>]]></content>
    
    
    <summary type="html">架构</summary>
    
    
    
    <category term="凤凰架构" scheme="http://universeinheart.github.io/categories/%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84/"/>
    
    
    <category term="架构" scheme="http://universeinheart.github.io/tags/%E6%9E%B6%E6%9E%84/"/>
    
    <category term="系统" scheme="http://universeinheart.github.io/tags/%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>从类库到服务</title>
    <link href="http://universeinheart.github.io/2022/04/06/%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84-%E6%9E%84%E5%BB%BA%E5%8F%AF%E9%9D%A0%E7%9A%84%E5%A4%A7%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/7/"/>
    <id>http://universeinheart.github.io/2022/04/06/%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84-%E6%9E%84%E5%BB%BA%E5%8F%AF%E9%9D%A0%E7%9A%84%E5%A4%A7%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/7/</id>
    <published>2022-04-06T15:26:37.000Z</published>
    <updated>2022-04-07T00:47:51.825Z</updated>
    
    <content type="html"><![CDATA[<p>微服务架构其中一个重要设计原则是“通过服务来实现独立自治的组件”，强调应采用<strong>“服务”（Service）</strong>而不再是“类库”（Library）来构建组件化的程序，这两者的差别在于类库是在编译期静态链接到程序中的，通过调用本地方法来使用其中的功能，而服务是进程外组件，通过调用<strong>远程</strong>方法来使用其中的功能。</p><p>微服务相互调用才能正常运作的分布式系统中，每个节点都互相扮演着服务的生产者与消费者的多重角色，形成了一套复杂的网状调用关系，此时，至少有（但不限于）以下三个问题是必须考虑并得到妥善解决的：</p><ul><li>对消费者来说，外部的服务由谁提供？具体在什么网络位置？<strong>（服务发现）</strong></li><li>对生产者来说，内部哪些服务需要暴露？哪些应当隐藏？应当以何种形式暴露服务？以什么规则在集群中分配请求？<strong>（服务的网关路由）</strong></li><li>对调用过程来说，如何保证每个远程服务都接收到相对平均的流量，获得尽可能高的服务质量与可靠性？<strong>（服务的负载均衡）</strong></li></ul><h2 id="服务发现"><a href="#服务发现" class="headerlink" title="服务发现"></a>服务发现</h2><h2 id="网关路由"><a href="#网关路由" class="headerlink" title="网关路由"></a>网关路由</h2><p>网关（Gateway） 用来表示位于内部区域边缘，与外界进行交互的某个物理或逻辑设备，</p><p>微服务中网关的首要职责就是作为统一的出口对外提供服务，将外部访问网关地址的流量，根据适当的规则路由到内部集群中正确的服务节点之上，因此，微服务中的网关，也常被称为“服务网关”或者“API 网关”，微服务中的网关首先应该是个路由器，在满足此前提的基础上，网关还可以根据需要作为流量过滤器来使用，提供某些额外的可选的功能，譬如安全、认证、授权、限流、监控、缓存，等等。简而言之</p><blockquote><p>网关 = 路由器（基础职能） + 过滤器（可选职能）</p></blockquote><p>针对“<strong>路由</strong>”这个基础职能，服务网关主要考量的是能够支持路由的“网络协议层次”和“性能与可用性”两方面的因素。</p><p><strong>网络协议层次</strong>是指负载均衡中介绍过的四层流量转发与七层流量代理，仅从技术实现角度来看，对于路由这项工作，负载均衡器与服务网关在实现上是没有什么差别的，很多服务网关本身就是基于老牌的负载均衡器来实现的，譬如基于 Nginx、HAProxy 开发的 Ingress Controller，基于 Netty 开发的 Zuul 2.0 等；但从目的角度看，负载均衡器与服务网关会有一些区别，具体在于前者是为了根据均衡算法对流量进行平均地路由，后者是为了根据流量中的某种特征进行正确地路由。网关必须能够识别流量中的特征，这意味着网关能够支持的网络通信协议的层次将会直接限制后端服务节点能够选择的服务通信方式。</p><blockquote><p>如果服务集群只提供像 Etcd 这样直接基于 TCP 的访问的服务，那只部署四层网关便可满足，网关以 IP 报文中源地址、目标地址为特征进行路由；</p><p>如果服务集群要提供 HTTP 服务的话，那就必须部署一个七层网关，网关根据 HTTP 报文中的 URL、Header 等信息为特征进行路由；</p><p>如果服务集群还要提供更上层的 WebSocket、SOAP 等服务，那就必须要求网关同样能够支持这些上层协议，才能从中提取到特征。</p></blockquote><p><strong>性能与可用性</strong>，由于网关是所有服务对外的总出口，是流量必经之地，所以网关的路由性能将导致全局的、系统性的影响，如果经过网关路由会有 1 毫秒的性能损失，就意味着整个系统所有服务的响应延迟都会增加 1 毫秒。网关的性能与它的工作模式和自身实现算法都有关系，但毫无疑问工作模式是最关键的因素，如果能够采用 DSR 三角传输模式，原理上就决定了性能一定会比代理模式来的强 </p><p>不过，因为今天 REST 和 JSON-RPC 等基于 HTTP 协议的服务接口在对外部提供的服务中占绝对主流的地位，所以我们所讨论的服务网关默认都必须支持七层路由，通常就默认无法直接进行流量转发，只能采用代理模式。在这个前提约束下，网关的性能主要取决于它们如何代理网络请求，也即它们的网络 I/O 模型，下面笔者正好借这个场景介绍一下网络 I/O 的基础知识。</p><h2 id="网络-I-O-模型"><a href="#网络-I-O-模型" class="headerlink" title="网络 I/O 模型"></a>网络 I/O 模型</h2><p>在套接字接口抽象下，网络 I/O 的出入口就是 <code>Socket</code> 的读和写，<code>Socket</code> 在操作系统接口中被抽象为数据流，网络 I/O 可以理解为对流的操作。每一次网络访问，从远程主机返回的数据会先存放到操作系统内核的缓冲区中，然后内核的缓冲区复制到应用程序的地址空间，所以当发生一次网络请求发生后，将会按顺序经历“<strong>等待数据从远程主机到达缓冲区</strong>”和“<strong>将数据从缓冲区拷贝到应用程序地址空间</strong>”两个阶段，根据实现这两个阶段的不同方法，人们把网络 I/O 模型总结为两类、五种模型：两类是指<strong>同步 I/O</strong>与<strong>异步 I/O</strong>，五种是指在同步 IO 中又分有划分出<strong>阻塞 I/O</strong>、<strong>非阻塞 I/O</strong>、<strong>多路复用 I/O</strong>和<strong>信号驱动 I/O</strong>四种细分模型以及<strong>异步I/O模型</strong>。</p><blockquote><p>同步是指调用端发出请求之后，得到结果之前必须一直等待，与之相对的就是异步，发出调用请求之后将立即返回，不会马上得到处理结果，结果将通过状态变化和回调来通知调用者。</p><p>阻塞和非阻塞是针对请求处理过程，指在收到调用请求之后，返回结果之前，当前处理线程是否会被挂起。</p></blockquote><p>以“如何领到外卖”为情景，将之类比解释如下：</p><ul><li><p><strong>异步 I/O</strong>（Asynchronous I/O）：好比你在美团外卖订了个盒饭，付款之后你自己该干嘛还干嘛去，饭做好了骑手自然会到门口打电话通知你。异步 I/O 中数据到达缓冲区后，不需要由调用进程主动进行从缓冲区复制数据的操作，而是复制完成后由操作系统向线程发送信号，所以它一定是非阻塞的。</p></li><li><p><strong>同步 I/O</strong></p><p>（Synchronous I/O）：好比你自己去饭堂打饭，这时可能有如下情形发生：</p><ul><li><strong>阻塞 I/O</strong>（Blocking I/O）：你去到饭堂，发现饭还没做好，你也干不了别的，只能打个瞌睡（线程休眠），直到饭做好，这就是被阻塞了。阻塞 I/O 是最直观的 I/O 模型，逻辑清晰，也比较节省 CPU 资源，但缺点就是线程休眠所带来的上下文切换，这是一种需要切换到内核态的重负载操作，不应当频繁进行。</li><li><strong>非阻塞 I/O</strong>（Non-Blocking I/O）：你去到饭堂，发现饭还没做好，你就回去了，然后每隔 3 分钟来一次饭堂看饭做好了没，直到饭做好。非阻塞 I/O 能够避免线程休眠，对于一些很快就能返回结果的请求，非阻塞 I/O 可以节省切换上下文切换的消耗，但是对于较长时间才能返回的请求，非阻塞 I/O 反而白白浪费了 CPU 资源，所以目前并不常用。</li><li><strong>多路复用 I/O</strong>（Multiplexing I/O）：多路复用 I/O 本质上是阻塞 I/O 的一种，但是它的好处是可以在同一条阻塞线程上处理多个不同端口的监听。类比的情景是你名字叫雷锋，代表整个宿舍去饭堂打饭，去到饭堂，发现饭还没做好，还是继续打瞌睡，但哪个舍友的饭好了，你就马上把那份饭送回去，然后继续打着瞌睡哼着歌等待其他的饭做好。多路复用 I/O 是目前的高并发网络应用的主流，它下面还可以细分 select、epoll、kqueue 等不同实现，这里就不作展开了。</li><li><strong>信号驱动 I/O</strong>（Signal-Driven I/O）：你去到饭堂，发现饭还没做好，但你跟厨师熟，跟他说饭做好了叫你，然后回去该干嘛干嘛，等收到厨师通知后，你把饭从饭堂拿回宿舍。这里厨师的通知就是那个“信号”，信号驱动 I/O 与异步 I/O 的区别是“从缓冲区获取数据”这个步骤的处理，前者收到的通知是可以开始进行复制操作了，即要你自己从饭堂拿回宿舍，在复制完成之前线程处于阻塞状态，所以它仍属于同步 I/O 操作，而后者收到的通知是复制操作已经完成，即外卖小哥已经把饭送到了。</li></ul></li></ul><p>显而易见，异步 I/O 模型是最方便的，毕竟能叫外卖谁愿意跑饭堂啊，但前提是你学校里有开展外卖业务。同样，异步 I/O 受限于操作系统，Windows NT 内核早在 3.5 以后，就通过<a href="https://zh.wikipedia.org/wiki/IOCP">IOCP</a>实现了真正的异步 I/O 模型。而 Linux 系统下，是在 Linux Kernel 2.6 才首次引入，目前也还并不算很完善，因此在 Linux 下实现高并发网络编程时仍是以多路复用 I/O 模型模式为主。</p><p>以 Zuul 网关为例，在 Zuul 1.0 时，它采用的是阻塞 I/O 模型来进行最经典的“一条线程对应一个连接”（Thread-per-Connection）的方式来代理流量，采用阻塞 I/O 意味着它会有线程休眠，就有上下文切换的成本，所以如果后端服务普遍属于<strong>计算密集型</strong>（CPU Bound，可以通俗理解为服务耗时比较长，主要消耗在 CPU 上）时，这种模式能够相对节省网关的 CPU 资源，但如果后端服务普遍都是 <strong>I/O 密集型</strong>（I/O Bound，可以理解服务都很快返回，主要消耗在 I/O 上），它就会由于频繁的上下文切换而降低性能。在 Zuul 的 2.0 版本，最大的改进就是基于 <code>Netty Server</code> 实现了异步 I/O 模型来处理请求，大幅度减少了线程数，获得了更高的性能和更低的延迟。</p><p><strong>网关的可用性问题</strong>，任何系统的网络调用过程中都至少会有一个单点存在，这是由用户只通过唯一的一个地址去访问系统所决定的。作为后端对外服务代理人角色的网关经常被视为整个系统的入口，往往很容易成为网络访问中的单点，这时候它的可用性就尤为重要。由于网关的地址具有唯一性，就不像之前<a href="https://icyfenix.cn/distribution/connect/service-discovery.html">服务发现</a>那些注册中心那样直接做个集群，随便访问哪一台都可以解决问题。为此，对网关的可用性方面，我们应该考虑到以下几点：</p><ul><li><strong>网关应尽可能轻量</strong>，尽管网关作为服务集群统一的出入口，可以很方便地做安全、认证、授权、限流、监控，等等的功能，但给网关附加这些能力时还是要仔细权衡，取得功能性与可用性之间的平衡，过度增加网关的职责是危险的。</li><li>网关选型时，应该尽可能选择较<strong>成熟的产品实现</strong>，譬如 Nginx Ingress Controller、KONG、Zuul 这些经受过长期考验的产品，而不能一味只考虑性能选择最新的产品，性能与可用性之间的平衡也需要权衡。</li><li>在需要高可用的生产环境中，应当考虑在网关之前部署负载均衡器或者<a href="https://en.wikipedia.org/wiki/Equal-cost_multi-path_routing">等价路由器</a>（ECMP），让那些更成熟健壮的设施（往往是硬件物理设备）去充当整个系统的入口地址，这样网关也可以进行扩展了</li></ul><h2 id="客户端负载均衡器"><a href="#客户端负载均衡器" class="headerlink" title="客户端负载均衡器"></a>客户端负载均衡器</h2><img src="/2022/04/06/%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84-%E6%9E%84%E5%BB%BA%E5%8F%AF%E9%9D%A0%E7%9A%84%E5%A4%A7%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/7/image-20220407083820770.png" class="" title="image-20220407083820770"><p>客户端均衡器是和服务实例一一对应的，而且与服务实例并存于同一个进程之内。这个特点能为它带来很多好处</p><ul><li>负载均衡器与服务之间信息交换是进程内的方法调用，不存在任何额外的网络开销。</li><li>不依赖集群边缘的设施，所有内部流量都仅在服务集群的内部循环，避免了出现前文那样，集群内部流量要“绕场一周”的尴尬局面。</li><li>分散式的均衡器意味着天然避免了集中式的单点问题，它的带宽资源将不会像集中式均衡器那样敏感</li><li>客户端均衡器要更加灵活，能够针对每一个服务实例单独设置均衡策略等参数，访问某个服务，是不是需要具备亲和性，选择服务的策略是随机、轮询、加权还是最小连接等等，都可以单独设置而不影响其它服务。</li></ul><p>它得到上述诸多好处的同时，缺点同样也是不少的：</p><ul><li>它与服务运行于同一个进程之内，意味着它的选型受到服务所使用的编程语言的限制，譬如用 Golang 开发的微服务就不太可能搭配 Spring Cloud Load Balancer 来使用，要为每种语言都实现对应的能够支持复杂网络情况的均衡器是非常难的。客户端均衡器的这个缺陷有违于微服务中技术异构不应受到限制的原则。</li><li>从个体服务来看，由于是共用一个进程，均衡器的稳定性会直接影响整个服务进程的稳定性，消耗的 CPU、内存等资源也同样影响到服务的可用资源。</li><li>由于请求的来源可能是来自集群中任意一个服务节点，而不再是统一来自集中式均衡器，这就使得内部网络安全和信任关系变得复杂，当攻破任何一个服务时，更容易通过该服务突破集群中的其他部分。</li><li>服务集群的拓扑关系是动态的，每一个客户端均衡器必须持续跟踪其他服务的健康状况，以实现上线新服务、下线旧服务、自动剔除失败的服务、自动重连恢复的服务等均衡器必须具备的功能。由于这些操作都需要通过访问服务注册中心来完成，数量庞大的客户端均衡器一直持续轮询服务注册中心，也会为它带来不小的负担。</li></ul><p>在 Java 领域，客户端均衡器中最具代表性的产品是 Netflix Ribbon 和 Spring Cloud Load Balancer，随着微服务的流行，它们在 Java 微服务中已积聚了相当可观的使用者。</p><h2 id="代理负载均衡器"><a href="#代理负载均衡器" class="headerlink" title="代理负载均衡器"></a>代理负载均衡器</h2><p>代理均衡器对此前的客户端负载均衡器的改进是将原本嵌入在服务进程中的均衡器提取出来，作为一个进程之外，同一 Pod 之内的特殊服务，放到<a href="https://icyfenix.cn/architecture/architect-history/post-microservices.html">边车代理</a>中去实现，它的流量关系如图所示。</p><img src="/2022/04/06/%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84-%E6%9E%84%E5%BB%BA%E5%8F%AF%E9%9D%A0%E7%9A%84%E5%A4%A7%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/7/image-20220407084422831.png" class="" title="image-20220407084422831"><p>虽然代理均衡器与服务实例不再是进程内通信，而是通过网络协议栈进行数据交换的，数据要经过操作系统的协议栈，要进行打包拆包、计算校验和、维护序列号等网络数据的收发步骤，流量比起之前的客户端均衡器确实多增加了一系列处理步骤。不过，Kubernetes 严格保证了同一个 Pod 中的容器不会跨越不同的节点，这些容器共享着同一个网络名称空间，因此代理均衡器与服务实例的交互，实质上是对本机回环设备的访问，仍然要比真正的网络交互高效且稳定得多。</p><p>代理均衡器付出的代价较小，但从服务进程中分离出来所获得的收益却是非常显著的：</p><ul><li>代理均衡器不再受编程语言的限制。集中不同编程语言的使用者的力量，更容易打造出能面对复杂网络情况的、高效健壮的均衡器。即使退一步说，独立于服务进程的均衡器也不会由于自身的稳定性影响到服务进程的稳定。</li><li>在服务拓扑感知方面代理均衡器也要更有优势。由于边车代理接受控制平面的统一管理，当服务节点拓扑关系发生变化时，控制平面就会主动向边车代理发送更新服务清单的控制指令，这避免了此前客户端均衡器必须长期主动轮询服务注册中心所造成的浪费。</li><li>在安全性、可观测性上，由于边车代理都是一致的实现，有利于在服务间建立双向 mTLS 通信，也有利于对整个调用链路给出更详细的统计信息。</li></ul><p>总体而言，边车代理这种通过同一个 Pod 的独立容器实现的负载均衡器是目前处理微服务集群内部流量最理想的方式，只是服务网格本身仍是初生事物，还不足够成熟，对操作系统、网络和运维方面的知识要求也较高，但有理由相信随着时间的推移，未来这将会是微服务的主流通信方式。</p>]]></content>
    
    
    <summary type="html">架构</summary>
    
    
    
    <category term="凤凰架构" scheme="http://universeinheart.github.io/categories/%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84/"/>
    
    
    <category term="架构" scheme="http://universeinheart.github.io/tags/%E6%9E%B6%E6%9E%84/"/>
    
    <category term="系统" scheme="http://universeinheart.github.io/tags/%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>33、Redis  脑裂：一次奇怪的数据丢失</title>
    <link href="http://universeinheart.github.io/2022/03/23/Redis/33/"/>
    <id>http://universeinheart.github.io/2022/03/23/Redis/33/</id>
    <published>2022-03-22T16:00:00.000Z</published>
    <updated>2022-03-11T03:19:54.974Z</updated>
    
    <content type="html"><![CDATA[<p>脑裂，就是指在主从集群中，同时有两个主节点，它们都能接收写请求。而脑裂最直接的影响，就是客户端不知道应该往哪个主节点写入数据，结果就是不同的客户端会往不同的主节点上写入数据。而且，严重的话，脑裂会进一步导致数据丢失。</p><h3 id="为什么会发生脑裂？"><a href="#为什么会发生脑裂？" class="headerlink" title="为什么会发生脑裂？"></a>为什么会发生脑裂？</h3><p>为什么数据会丢失？是不是数据同步出了问题？</p><p><strong>第一步：确认是不是数据同步出现了问题</strong></p><p>在主从集群中发生数据丢失，最常见的原因就是**主库的数据还没有同步到从库，结果主库发生了故障，等从库升级为主库后，未同步的数据就丢失了。 </p><p>如果是这种情况的数据丢失，我们可以通过比对主从库上的复制进度差值来进行判断，也就是计算 <code>master_repl_offset</code> 和 <code>slave_repl_offset</code> 的差值。如果从库上的 <code>slave_repl_offset</code> 小于原主库的 <code>master_repl_offset</code>，那么，我们就可以认定数据丢失是由数据同步未完成导致的。</p><p><strong>第二步：排查客户端的操作日志，发现脑裂现象</strong></p><p>在排查客户端的操作日志时，我们发现，在主从切换后的一段时间内，有一个客户端仍然在和原主库通信，并没有和升级的新主库进行交互。这就相当于主从集群中同时有了两个主库。根据这个迹象，我们就想到了在分布式主从集群发生故障时会出现的一个问题：脑裂。</p><p>但是，不同客户端给两个主库发送数据写操作，按道理来说，只会导致新数据会分布在不同的主库上，并不会造成数据丢失。那么，为什么我们的数据仍然丢失了呢？</p><p><strong>第三步：发现是原主库假故障导致的脑裂</strong></p><p>我们是采用哨兵机制进行主从切换的，当主从切换发生时，一定是有超过预设数量（quorum 配置项）的哨兵实例和主库的心跳都超时了，才会把主库判断为客观下线，然后，哨兵开始执行切换操作。哨兵切换完成后，客户端会和新主库进行通信，发送请求操作。</p><p>但是，在切换过程中，既然客户端仍然和原主库通信，这就表明，原主库并没有真的发生故障（例如主库进程挂掉）。我们猜测，主库是由于某些原因无法处理请求，也没有响应哨兵的心跳，才<strong>被哨兵错误地判断为客观下线的</strong>。结果，在被判断下线之后，原主库又重新开始处理请求了，而此时，哨兵还没有完成主从切换，客户端仍然可以和原主库通信，客户端发送的写操作就会在原主库上写入数据了。</p><h3 id="为什么脑裂会导致数据丢失？"><a href="#为什么脑裂会导致数据丢失？" class="headerlink" title="为什么脑裂会导致数据丢失？"></a>为什么脑裂会导致数据丢失？</h3><p>主从切换后，从库一旦升级为新主库，哨兵就会让原主库执行 slave of 命令，和新主库重新进行全量同步。而在全量同步执行的最后阶段，原主库需要清空本地的数据，加载新主库发送的 RDB 文件，这样一来，原主库在主从切换期间保存的新写数据就丢失了。</p><p>在主从切换的过程中，如果原主库只是“假故障”，它会触发哨兵启动主从切换，一旦等它从假故障中恢复后，又开始处理请求，这样一来，就会和新主库同时存在，形成脑裂。等到哨兵让原主库和新主库做全量同步后，原主库在切换期间保存的数据就丢失了。</p><h3 id="如何应对脑裂问题？"><a href="#如何应对脑裂问题？" class="headerlink" title="如何应对脑裂问题？"></a>如何应对脑裂问题？</h3><p>Redis 已经提供了两个配置项来限制主库的请求处理，分别是 <code>min-slaves-to-write</code> 和 <code>min-slaves-max-lag</code>。</p><ul><li><code>min-slaves-to-write</code>：这个配置项设置了主库能进行数据同步的最少从库数量；</li><li><code>min-slaves-max-lag</code>：这个配置项设置了主从库间进行数据复制时，从库给主库发送 ACK 消息的最大延迟（以秒为单位）。</li></ul><p>主库连接的从库中至少有 N 个从库，和主库进行数据复制时的 ACK 消息延迟不能超过 T 秒，否则，主库就不会再接收客户端的请求了。即使原主库是假故障，它在假故障期间也无法响应哨兵心跳，也不能和从库进行同步，自然也就无法和从库进行 ACK 确认了。这样一来，min-slaves-to-write 和 min-slaves-max-lag 的组合要求就无法得到满足，原主库就会被限制接收客户端请求，客户端也就不能在原主库中写入新数据了。等到新主库上线时，就只有新主库能接收和处理客户端请求，此时，新写的数据会被直接写到新主库中。而原主库会被哨兵降为从库，即使它的数据被清空了，也不会有新数据丢失。</p><blockquote><p>例子: 假设我们将 min-slaves-to-write 设置为 1，把 min-slaves-max-lag 设置为 12s，把哨兵的 down-after-milliseconds 设置为 10s，主库因为某些原因卡住了 15s，导致哨兵判断主库客观下线，开始进行主从切换。同时，因为原主库卡住了 15s，没有一个从库能和原主库在 12s 内进行数据复制，原主库也无法接收客户端请求了。这样一来，主从切换完成后，也只有新主库能接收请求，不会发生脑裂，也就不会发生数据丢失的问题了。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;脑裂，就是指在主从集群中，同时有两个主节点，它们都能接收写请求。而脑裂最直接的影响，就是客户端不知道应该往哪个主节点写入数据，结果就是不同的客户端会往不同的主节点上写入数据。而且，严重的话，脑裂会进一步导致数据丢失。&lt;/p&gt;
&lt;h3 id=&quot;为什么会发生脑裂？&quot;&gt;&lt;a hr</summary>
      
    
    
    
    <category term="极客时间Redis实战" scheme="http://universeinheart.github.io/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4Redis%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="Redis" scheme="http://universeinheart.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>32、Redis Redis主从同步与故障切换，有哪些坑？</title>
    <link href="http://universeinheart.github.io/2022/03/22/Redis/32/"/>
    <id>http://universeinheart.github.io/2022/03/22/Redis/32/</id>
    <published>2022-03-21T16:00:00.000Z</published>
    <updated>2022-03-11T02:22:52.730Z</updated>
    
    <content type="html"><![CDATA[<h3 id="主从数据不一致"><a href="#主从数据不一致" class="headerlink" title="主从数据不一致"></a>主从数据不一致</h3><p>主从库间的命令复制是异步进行的。具体来说，在主从库命令传播阶段，主库收到新的写命令后，会发送给从库。但是，主库并不会等到从库实际执行完命令后，再把结果返回给客户端，而是主库自己在本地执行完命令后，就会向客户端返回结果了。如果从库还没有执行主库同步过来的命令，主从库间的数据就不一致了。</p><blockquote><p><strong>那在什么情况下，从库会滞后执行同步命令呢？</strong></p><p>一方面，主从库间的网络可能会有传输延迟，所以从库不能及时地收到主库发送的命令，从库上执行同步命令的时间就会被延后。</p><p>另一方面，即使从库及时收到了主库的命令，但是，也可能会因为正在处理其它复杂度高的命令（例如集合操作命令）而阻塞。此时，从库需要处理完当前的命令，才能执行主库发送的命令操作，这就会造成主从数据不一致。 </p></blockquote><p>在硬件环境配置方面，我们要尽量保证主从库间的网络连接状况良好。</p><p>Redis 的 INFO replication 命令可以查看主库接收写命令的进度信息（master_repl_offset）和从库复制写命令的进度信息（slave_repl_offset）， 先用 INFO replication 命令查到主、从库的进度，然后，我们用 master_repl_offset 减去 slave_repl_offset，这样就能得到从库和主库间的复制进度差值了。</p><h3 id="读取过期数据"><a href="#读取过期数据" class="headerlink" title="读取过期数据"></a>读取过期数据</h3><p>Redis 同时使用了两种策略来删除过期的数据，分别是<strong>惰性删除策略</strong>和<strong>定期删除策略</strong>。</p><p><strong>惰性删除策略</strong>。当一个数据的过期时间到了以后，并不会立即删除数据，而是等到再有请求来读写这个数据时，对数据进行检查，如果发现数据已经过期了，再删除这个数据。这个策略的好处是尽量<strong>减少删除操作对 CPU 资源的使用</strong>，对于用不到的数据，就不再浪费时间进行检查和删除了。但是，这个策略会导致大量已经过期的数据留存在内存中，<strong>占用较多的内存资源</strong>。</p><p><strong>定期删除策略</strong>是指，Redis 每隔一段时间（默认 100ms），就会随机选出一定数量的数据，检查它们是否过期，并把其中过期的数据删除，这样就可以及时释放一些内存。</p><p>如果你使用的是 Redis 3.2 之前的版本，那么，从库在服务读请求时，并不会判断数据是否过期，而是会返回过期数据。在 3.2 版本后，Redis 做了改进，如果读取的数据已经过期了，从库虽然不会删除，但是会返回空值，这就避免了客户端读到过期数据。所以，在应用主从集群时，尽量使用 Redis 3.2 及以上版本。</p><p>设置数据过期时间的命令一共有 4 个，我们可以把它们分成两类：</p><ol><li><code>EXPIRE</code> 和 <code>PEXPIRE</code>：它们给数据设置的是从命令执行时开始计算的存活时间；</li><li><code>EXPIREAT</code> 和 <code>PEXPIREAT</code>：它们会直接把数据的过期时间设置为具体的一个时间点。</li></ol><img src="/2022/03/22/Redis/32/image-20220311101520174.png" class="" title="image-20220311101520174"><p><strong>在业务应用中使用 <code>EXPIREAT/PEXPIREAT</code> 命令，把数据的过期时间设置为具体的时间点，避免读到过期数据。</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;主从数据不一致&quot;&gt;&lt;a href=&quot;#主从数据不一致&quot; class=&quot;headerlink&quot; title=&quot;主从数据不一致&quot;&gt;&lt;/a&gt;主从数据不一致&lt;/h3&gt;&lt;p&gt;主从库间的命令复制是异步进行的。具体来说，在主从库命令传播阶段，主库收到新的写命令后，会发送给从库。</summary>
      
    
    
    
    <category term="极客时间Redis实战" scheme="http://universeinheart.github.io/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4Redis%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="Redis" scheme="http://universeinheart.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>32、Redis Redis主从同步与故障切换，有哪些坑？</title>
    <link href="http://universeinheart.github.io/2022/03/22/Redis/33%20-%20%E5%89%AF%E6%9C%AC%20(5)/"/>
    <id>http://universeinheart.github.io/2022/03/22/Redis/33%20-%20%E5%89%AF%E6%9C%AC%20(5)/</id>
    <published>2022-03-21T16:00:00.000Z</published>
    <updated>2022-03-10T08:22:05.449Z</updated>
    
    
    
    
    <category term="极客时间Redis实战" scheme="http://universeinheart.github.io/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4Redis%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="Redis" scheme="http://universeinheart.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>32、Redis Redis主从同步与故障切换，有哪些坑？</title>
    <link href="http://universeinheart.github.io/2022/03/22/Redis/33%20-%20%E5%89%AF%E6%9C%AC%20(2)/"/>
    <id>http://universeinheart.github.io/2022/03/22/Redis/33%20-%20%E5%89%AF%E6%9C%AC%20(2)/</id>
    <published>2022-03-21T16:00:00.000Z</published>
    <updated>2022-03-10T08:22:05.449Z</updated>
    
    
    
    
    <category term="极客时间Redis实战" scheme="http://universeinheart.github.io/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4Redis%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="Redis" scheme="http://universeinheart.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>32、Redis Redis主从同步与故障切换，有哪些坑？</title>
    <link href="http://universeinheart.github.io/2022/03/22/Redis/33%20-%20%E5%89%AF%E6%9C%AC%20(3)/"/>
    <id>http://universeinheart.github.io/2022/03/22/Redis/33%20-%20%E5%89%AF%E6%9C%AC%20(3)/</id>
    <published>2022-03-21T16:00:00.000Z</published>
    <updated>2022-03-10T08:22:05.449Z</updated>
    
    
    
    
    <category term="极客时间Redis实战" scheme="http://universeinheart.github.io/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4Redis%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="Redis" scheme="http://universeinheart.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>32、Redis Redis主从同步与故障切换，有哪些坑？</title>
    <link href="http://universeinheart.github.io/2022/03/22/Redis/33%20-%20%E5%89%AF%E6%9C%AC%20(6)/"/>
    <id>http://universeinheart.github.io/2022/03/22/Redis/33%20-%20%E5%89%AF%E6%9C%AC%20(6)/</id>
    <published>2022-03-21T16:00:00.000Z</published>
    <updated>2022-03-10T08:22:05.449Z</updated>
    
    
    
    
    <category term="极客时间Redis实战" scheme="http://universeinheart.github.io/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4Redis%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="Redis" scheme="http://universeinheart.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>32、Redis Redis主从同步与故障切换，有哪些坑？</title>
    <link href="http://universeinheart.github.io/2022/03/22/Redis/33%20-%20%E5%89%AF%E6%9C%AC/"/>
    <id>http://universeinheart.github.io/2022/03/22/Redis/33%20-%20%E5%89%AF%E6%9C%AC/</id>
    <published>2022-03-21T16:00:00.000Z</published>
    <updated>2022-03-10T08:22:05.449Z</updated>
    
    
    
    
    <category term="极客时间Redis实战" scheme="http://universeinheart.github.io/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4Redis%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="Redis" scheme="http://universeinheart.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>32、Redis Redis主从同步与故障切换，有哪些坑？</title>
    <link href="http://universeinheart.github.io/2022/03/22/Redis/33%20-%20%E5%89%AF%E6%9C%AC%20(4)/"/>
    <id>http://universeinheart.github.io/2022/03/22/Redis/33%20-%20%E5%89%AF%E6%9C%AC%20(4)/</id>
    <published>2022-03-21T16:00:00.000Z</published>
    <updated>2022-03-10T08:22:05.449Z</updated>
    
    
    
    
    <category term="极客时间Redis实战" scheme="http://universeinheart.github.io/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4Redis%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="Redis" scheme="http://universeinheart.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>31、Redis 事务机制：Redis能实现ACID属性吗？</title>
    <link href="http://universeinheart.github.io/2022/03/21/Redis/31/"/>
    <id>http://universeinheart.github.io/2022/03/21/Redis/31/</id>
    <published>2022-03-20T16:00:00.000Z</published>
    <updated>2022-03-10T03:19:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>事务是数据库的一个重要功能。所谓的事务，就是指对数据进行读写的一系列操作。事务在执行时，会提供专门的属性保证，包括<strong>原子性（Atomicity）</strong>、<strong>一致性（Consistency）</strong>、<strong>隔离性（Isolation）</strong>和<strong>持久性（Durability）</strong>，也就是 ACID 属性。这些属性既包括了对事务执行结果的要求，也有对数据库在事务执行前后的数据状态变化的要求。</p><p>略</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Redis 通过 <code>MULTI</code>、<code>EXEC</code>、<code>DISCARD</code> 和 <code>WATCH</code> 四个命令来支持事务机制</p><img src="/2022/03/21/Redis/31/image-20220310111641850.png" class="" title="image-20220310111641850"><p>Redis 的事务机制可以保证一致性和隔离性，但是无法保证持久性。不过，因为 Redis 本身是内存数据库，持久性并不是一个必须的属性，原子性的情况比较复杂，只有当事务中使用的命令语法有误时，原子性得不到保证，在其它情况下，事务都可以原子性执行。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;事务是数据库的一个重要功能。所谓的事务，就是指对数据进行读写的一系列操作。事务在执行时，会提供专门的属性保证，包括&lt;strong&gt;原子性（Atomicity）&lt;/strong&gt;、&lt;strong&gt;一致性（Consistency）&lt;/strong&gt;、&lt;strong&gt;隔离性（Is</summary>
      
    
    
    
    <category term="极客时间Redis实战" scheme="http://universeinheart.github.io/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4Redis%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="Redis" scheme="http://universeinheart.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>30、Redis 如何使用Redis实现分布式锁？</title>
    <link href="http://universeinheart.github.io/2022/03/20/Redis/30/"/>
    <id>http://universeinheart.github.io/2022/03/20/Redis/30/</id>
    <published>2022-03-19T16:00:00.000Z</published>
    <updated>2022-03-10T03:08:33.127Z</updated>
    
    <content type="html"><![CDATA[<p>Redis 属于分布式系统，当有多个客户端需要争抢锁时，我们必须要保证，这把锁不能是某个客户端本地的锁。否则的话，其它客户端是无法访问这把锁的，当然也就不能获取这把锁了。所以，在分布式系统中，当有多个客户端需要获取锁时，我们需要分布式锁。此时，锁是保存在一个共享存储系统中的，可以被多个客户端共享访问和获取。</p><p>在分布式场景下，<strong>锁变量需要由一个共享存储系统来维护</strong>，只有这样，多个客户端才可以通过访问共享存储系统来访问锁变量。相应的，<strong>加锁和释放锁的操作就变成了读取、判断和设置共享存储系统中的锁变量值</strong>。</p><h3 id="基于单个-Redis-节点实现分布式锁"><a href="#基于单个-Redis-节点实现分布式锁" class="headerlink" title="基于单个 Redis 节点实现分布式锁"></a>基于单个 Redis 节点实现分布式锁</h3><img src="/2022/03/20/Redis/30/image-20220310105457712.png" class="" title="image-20220310105457712"><p>加锁包含了三个操作（读取锁变量、判断锁变量值以及把锁变量值设置为 1），而这三个操作在执行时需要保证原子性。 <code>SETNX</code> 命令，它用于设置键值对的值。使用 <code>DEL</code> 命令删除锁变量</p><blockquote><p>使用 SETNX 和 DEL 命令组合实现分布锁，存在两个潜在的风险。</p><p>第一个风险是，假如某个客户端在执行了 SETNX 命令、加锁之后，紧接着却在操作共享数据时发生了异常，结果一直没有执行最后的 DEL 命令释放锁。因此，锁就一直被这个客户端持有，其它客户端无法拿到锁，也无法访问共享数据和执行后续操作，这会给业务应用带来影响。针对这个问题，一个有效的解决方法是，<strong>给锁变量设置一个过期时间</strong>。</p><p>第二个风险。如果客户端 A 执行了 SETNX 命令加锁后，假设客户端 B 执行了 DEL 命令释放锁，此时，客户端 A 的锁就被误释放了。如果客户端 C 正好也在申请加锁，就可以成功获得锁，进而开始操作共享数据。这样一来，客户端 A 和 C 同时在对共享数据进行操作，数据就会被修改错误，这也是业务层不能接受的。<strong>我们需要能区分来自不同客户端的锁操作</strong>。</p></blockquote><h3 id="基于多个-Redis-节点实现高可靠的分布式锁"><a href="#基于多个-Redis-节点实现高可靠的分布式锁" class="headerlink" title="基于多个 Redis 节点实现高可靠的分布式锁"></a>基于多个 Redis 节点实现高可靠的分布式锁</h3><p>为了避免 Redis 实例故障而导致的锁无法工作的问题，Redis 的开发者 Antirez 提出了分布式锁算法 Redlock。</p><p>Redlock 算法的基本思路，是让客户端和多个独立的 Redis 实例依次请求加锁，如果客户端能够和半数以上的实例成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁了，否则加锁失败。这样一来，即使有单个 Redis 实例发生故障，因为锁变量在其它实例上也有保存，所以，客户端仍然可以正常地进行锁操作，锁变量并不会丢失。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Redis 属于分布式系统，当有多个客户端需要争抢锁时，我们必须要保证，这把锁不能是某个客户端本地的锁。否则的话，其它客户端是无法访问这把锁的，当然也就不能获取这把锁了。所以，在分布式系统中，当有多个客户端需要获取锁时，我们需要分布式锁。此时，锁是保存在一个共享存储系统中的</summary>
      
    
    
    
    <category term="极客时间Redis实战" scheme="http://universeinheart.github.io/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4Redis%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="Redis" scheme="http://universeinheart.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>29、Redis 无锁的原子操作：Redis如何应对并发访问？</title>
    <link href="http://universeinheart.github.io/2022/03/19/Redis/29/"/>
    <id>http://universeinheart.github.io/2022/03/19/Redis/29/</id>
    <published>2022-03-18T16:00:00.000Z</published>
    <updated>2022-03-09T02:55:52.104Z</updated>
    
    <content type="html"><![CDATA[<p>在使用 Redis 时，不可避免地会遇到<strong>并发访问</strong>的问题，比如说如果多个用户同时下单，就会对缓存在 Redis 中的商品库存并发更新。一旦有了并发写操作，数据就会被修改，如果我们没有对并发写请求做好控制，就可能导致数据被改错，影响到业务的正常使用</p><p>为了保证并发访问的正确性，Redis 提供了两种方法，分别是<strong>加锁</strong>和<strong>原子操作</strong>。</p><p><strong>加锁</strong>是一种常用的方法，在读取数据前，客户端需要先获得锁，否则就无法进行操作。当一个客户端获得锁后，就会一直持有这把锁，直到客户端完成数据更新，才释放这把锁。但是，其实这里会有两个问题：一个是，如果加锁操作多，<strong>会降低系统的并发访问性能</strong>；第二个是，Redis 客户端要加锁时，需要用到<strong>分布式锁</strong>，而分布式锁实现复杂，需要用额外的存储系统来提供加解锁操作</p><p><strong>原子操作</strong>是指执行过程保持原子性的操作，而且原子操作执行时并不需要再加锁，实现了无锁操作。这样一来，既能保证并发控制，还能减少对系统并发性能的影响。</p><h2 id="并发访问中需要对什么进行控制？"><a href="#并发访问中需要对什么进行控制？" class="headerlink" title="并发访问中需要对什么进行控制？"></a>并发访问中需要对什么进行控制？</h2><p>并发访问控制对应的操作主要是数据修改操作。当客户端需要修改数据时，基本流程分成两步：1、客户端先把数据读取到本地，在本地进行修改；2、客户端修改完数据后，再写回 Redis。我们把这个流程叫做“读取 - 修改 - 写回”操作（Read-Modify-Write，简称为 RMW 操作）。</p><p>当有多个客户端对同一份数据执行 RMW 操作的话，我们就需要让 RMW 操作涉及的代码以原子性方式执行。访问同一份数据的 RMW 操作代码，就叫做临界区代码。</p><p>为了保证数据并发修改的正确性，我们可以用锁把并行操作变成串行操作，串行操作就具有互斥性。一个客户端持有锁后，其他客户端只能等到锁释放，才能拿锁再进行修改。但是加锁也会导致系统并发性能降低。原子操作也能实现并发控制，但是原子操作对系统并发性能的影响较小。</p><h2 id="Redis-的两种原子操作方法"><a href="#Redis-的两种原子操作方法" class="headerlink" title="Redis 的两种原子操作方法"></a>Redis 的两种原子操作方法</h2><p>为了实现并发控制要求的临界区代码互斥执行，Redis 的原子操作采用了两种方法：</p><ol><li>把多个操作在 Redis 中实现成一个操作，也就是<strong>单命令操作</strong>；</li><li>把多个操作写到一个 <strong>Lua 脚本</strong>中，以原子性方式执行单个 Lua 脚本。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在使用 Redis 时，不可避免地会遇到&lt;strong&gt;并发访问&lt;/strong&gt;的问题，比如说如果多个用户同时下单，就会对缓存在 Redis 中的商品库存并发更新。一旦有了并发写操作，数据就会被修改，如果我们没有对并发写请求做好控制，就可能导致数据被改错，影响到业务的正常</summary>
      
    
    
    
    <category term="极客时间Redis实战" scheme="http://universeinheart.github.io/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4Redis%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="Redis" scheme="http://universeinheart.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>28、Redis Pika：如何基于SSD实现大容量Redis？</title>
    <link href="http://universeinheart.github.io/2022/03/18/Redis/28/"/>
    <id>http://universeinheart.github.io/2022/03/18/Redis/28/</id>
    <published>2022-03-17T16:00:00.000Z</published>
    <updated>2022-03-08T08:48:08.667Z</updated>
    
    <content type="html"><![CDATA[<p>基于 SSD 来实现大容量的 Redis 实例。360 公司 DBA 和基础架构组联合开发的 <code>Pika</code> 键值数据库</p><p>Pika 在刚开始设计的时候，就有两个目标：一是，单实例可以保存大容量数据，同时避免了实例恢复和主从同步时的潜在问题，例如恢复时间增长、主从切换开销大、缓冲区易溢出。；二是，和 Redis 数据类型保持兼容，可以支持使用 Redis 的应用平滑地迁移到 Pika 上。所以，如果你一直在使用 Redis，并且想使用 SSD 来扩展单实例容量，Pika 就是一个很好的选择。</p><h2 id="大内存-Redis-实例的潜在问题"><a href="#大内存-Redis-实例的潜在问题" class="headerlink" title="大内存 Redis 实例的潜在问题"></a>大内存 Redis 实例的潜在问题</h2><p><strong>内存快照 RDB 生成和恢复效率低</strong></p><p>内存大小和内存快照 RDB 的关系是非常直接的：实例内存容量大，RDB 文件也会相应增大，那么，RDB 文件生成时的 fork 时长就会增加，这就会导致 Redis 实例阻塞。而且，RDB 文件增大后，使用 RDB 进行恢复的时长也会增加，会导致 Redis 较长时间无法对外提供服务。</p><p><strong>主从节点全量同步时长增加</strong></p><p>主从节点间的同步的第一步就是要做全量同步。全量同步是主节点生成 RDB 文件，并传给从节点，从节点再进行加载。试想一下，如果 RDB 文件很大，肯定会导致全量同步的时长增加，效率不高，而且还可能会导致复制缓冲区溢出。</p><p><strong>缓冲区易溢出</strong></p><p>一旦缓冲区溢出了，主从节点间就会又开始全量同步，影响业务应用的正常使用。如果我们增加复制缓冲区的容量，这又会消耗宝贵的内存资源。</p><h2 id="Pika-的整体架构"><a href="#Pika-的整体架构" class="headerlink" title="Pika 的整体架构"></a>Pika 的整体架构</h2><p>Pika 键值数据库的整体架构中包括了五部分，分别是<strong>网络框架</strong>、<strong>Pika 线程模块</strong>、<strong>Nemo 存储模块</strong>、<strong>RocksDB</strong> 和 <strong>binlog 机制</strong>，如下图所示：</p><img src="/2022/03/18/Redis/28/image-20220308154319251.png" class="" title="image-20220308154319251"><p><strong>网络框架</strong>主要负责底层网络请求的接收和发送。Pika 的网络框架是对操作系统底层的网络函数进行了封装。Pika 在进行网络通信时，可以直接调用网络框架封装好的函数。</p><p><strong>Pika 线程模块</strong>采用了多线程模型来具体处理客户端请求，包括一个请求分发线程（<code>DispatchThread</code>）、一组工作线程（<code>WorkerThread</code>）以及一个线程池（<code>ThreadPool</code>）。</p><p>请求分发线程专门监听网络端口，一旦接收到客户端的连接请求后，就和客户端建立连接，并把连接交由工作线程处理。工作线程负责接收客户端连接上发送的具体命令请求，并把命令请求封装成 Task，再交给线程池中的线程，由这些线程进行实际的数据存取处理，如下图所示：</p><img src="/2022/03/18/Redis/28/image-20220308160006358.png" class="" title="image-20220308160006358"><p><strong>Nemo 模块</strong>很容易理解，它实现了 Pika 和 Redis 的数据类型兼容。这样一来，当我们把 Redis 服务迁移到 Pika 时，不用修改业务应用中操作 Redis 的代码，而且还可以继续应用运维 Redis 的经验，这使得 Pika 的学习成本就较低</p><p><strong>RocksDB</strong> 提供的基于 SSD 保存数据的功能。它使得 Pika 可以不用大容量的内存，就能保存更多数据，还避免了使用内存快照。而且，Pika 使用 <strong>binlog 机制</strong>记录写命令，用于主从节点的命令同步，避免了刚刚所说的大内存实例在主从同步过程中的潜在问题。</p><h2 id="Pika-如何基于-SSD-保存更多数据？"><a href="#Pika-如何基于-SSD-保存更多数据？" class="headerlink" title="Pika 如何基于 SSD 保存更多数据？"></a>Pika 如何基于 SSD 保存更多数据？</h2><p>为了把数据保存到 SSD，Pika 使用了业界广泛应用的持久化键值数据库 <code>RocksDB</code>。</p><p><strong>RocksDB 的基本数据读写机制</strong></p><img src="/2022/03/18/Redis/28/image-20220308163030639.png" class="" title="image-20220308163030639"><p>当 <code>Pika</code> 需要<strong>保存数据</strong>时，<code>RocksDB</code> 会使用两小块内存空间（<code>Memtable1</code> 和 <code>Memtable2</code>）来交替缓存写入的数据。<code>Memtable</code> 的大小可以设置，一个 Memtable 的大小一般为几 MB 或几十 MB。当有数据要写入 <code>RocksDB</code> 时，<code>RocksDB</code> 会先把数据写入到 <code>Memtable1</code>。等到 Memtable1 写满后，<code>RocksDB</code> 再把数据以文件的形式，快速写入底层的 SSD。同时，RocksDB 会使用 <code>Memtable2</code> 来代替 <code>Memtable1</code>，缓存新写入的数据。等到 <code>Memtable1</code> 的数据都写入 SSD 了，RocksDB 会在 <code>Memtable2</code> 写满后，再用 <code>Memtable1</code> 缓存新写入的数据。</p><p>当 Pika 需要<strong>读取数据</strong>的时候，<code>RocksDB</code> 会先在 <code>Memtable</code> 中查询是否有要读取的数据。这是因为，最新的数据都是先写入到 <code>Memtable</code> 中的。如果 <code>Memtable</code> 中没有要读取的数据，<code>RocksDB</code> 会再查询保存在 SSD 上的数据文件</p><blockquote><p>当使用大内存实例保存大量数据时，Redis 会面临 RDB 生成和恢复的效率问题，以及主从同步时的效率和缓冲区溢出问题。那么，当 Pika 保存大量数据时，不会存在相同的问题</p><p>一方面，Pika 基于 RocksDB 保存了数据文件，直接读取数据文件就能恢复，不需要再通过内存快照进行恢复了。而且，Pika 从库在进行全量同步时，可以直接从主库拷贝数据文件，不需要使用内存快照，这样一来，Pika 就避免了大内存快照生成效率低的问题。</p><p>另一方面，Pika 使用了 binlog 机制实现增量命令同步，既节省了内存，还避免了缓冲区溢出的问题。binlog 是保存在 SSD 上的文件，Pika 接收到写命令后，在把数据写入 Memtable 时，也会把命令操作写到 binlog 文件中。和 Redis 类似，当全量同步结束后，从库会从 binlog 中把尚未同步的命令读取过来，这样就可以和主库的数据保持一致。当进行增量同步时，从库也是把自己已经复制的偏移量发给主库，主库把尚未同步的命令发给从库，来保持主从库的数据一致。</p><p>和 Redis 使用缓冲区相比，使用 binlog 好处是非常明显的：binlog 是保存在 SSD 上的文件，文件大小不像缓冲区，会受到内存容量的较多限制。而且，当 binlog 文件增大后，还可以通过轮替操作，生成新的 binlog 文件，再把旧的 binlog 文件独立保存。这样一来，即使 Pika 实例保存了大量的数据，在同步过程中也不会出现缓冲区溢出的问题了。</p></blockquote><h2 id="Pika-如何实现-Redis-数据类型兼容？"><a href="#Pika-如何实现-Redis-数据类型兼容？" class="headerlink" title="Pika 如何实现 Redis 数据类型兼容？"></a>Pika 如何实现 Redis 数据类型兼容？</h2><h2 id="Pika-的其他优势与不足"><a href="#Pika-的其他优势与不足" class="headerlink" title="Pika 的其他优势与不足"></a>Pika 的其他优势与不足</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;基于 SSD 来实现大容量的 Redis 实例。360 公司 DBA 和基础架构组联合开发的 &lt;code&gt;Pika&lt;/code&gt; 键值数据库&lt;/p&gt;
&lt;p&gt;Pika 在刚开始设计的时候，就有两个目标：一是，单实例可以保存大容量数据，同时避免了实例恢复和主从同步时的潜在问题，</summary>
      
    
    
    
    <category term="极客时间Redis实战" scheme="http://universeinheart.github.io/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4Redis%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="Redis" scheme="http://universeinheart.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>27、Redis 缓存被污染了，该怎么办？</title>
    <link href="http://universeinheart.github.io/2022/03/17/Redis/27/"/>
    <id>http://universeinheart.github.io/2022/03/17/Redis/27/</id>
    <published>2022-03-16T16:00:00.000Z</published>
    <updated>2022-03-08T06:21:04.615Z</updated>
    
    <content type="html"><![CDATA[<p>在一些场景下，有些数据被访问的次数非常少，甚至只会被访问一次。当这些数据服务完访问请求后，如果还继续留存在缓存中的话，就只会白白占用缓存空间。这种情况，就是缓存污染。</p><h2 id="如何解决缓存污染问题？"><a href="#如何解决缓存污染问题？" class="headerlink" title="如何解决缓存污染问题？"></a>如何解决缓存污染问题？</h2><p>把不会再被访问的数据筛选出来并淘汰掉。这样就不用等到缓存被写满以后，再逐一淘汰旧数据之后，才能写入新数据了。而哪些数据能留存在缓存中，是由缓存的淘汰策略决定的。</p><h2 id="LRU-缓存策略"><a href="#LRU-缓存策略" class="headerlink" title="LRU 缓存策略"></a>LRU 缓存策略</h2><p>LRU 策略的核心思想：如果一个数据刚刚被访问，那么这个数据肯定是热数据，还会被再次访问。</p><p>按照这个核心思想，Redis 中的 LRU 策略，会在每个数据对应的 RedisObject 结构体中设置一个 lru 字段，用来记录数据的访问时间戳。在进行数据淘汰时，LRU 策略会在候选数据集中淘汰掉 lru 字段值最小的数据（也就是访问时间最久的数据）。</p><p>只看数据的访问时间，使用 LRU 策略在处理<strong>扫描式单次查询操作</strong>时，无法解决缓存污染，在使用 LRU 策略淘汰数据时，这些数据会留存在缓存中很长一段时间，造成缓存污染。</p><h2 id="LFU-缓存策略的优化"><a href="#LFU-缓存策略的优化" class="headerlink" title="LFU 缓存策略的优化"></a>LFU 缓存策略的优化</h2><p>LFU 缓存策略是在 LRU 策略基础上，为每个数据增加了一个计数器，来统计这个数据的<strong>访问次数</strong>。当使用 LFU 策略筛选淘汰数据时，首先会根据数据的访问次数进行筛选，把访问次数最低的数据淘汰出缓存。如果两个数据的访问次数相同，LFU 策略再比较这两个数据的访问时效性，把距离上一次访问时间更久的数据淘汰出缓存。</p><p>Redis 在实现 LFU 策略的时候，RedisObject 结构中设置了两个字段，<strong>ldt 值</strong>：表示数据的访问时间戳；<strong>counter 值</strong>：表示数据的访问次数。</p><p>在实现 LFU 策略时，Redis 并没有采用数据每被访问一次，就给对应的 counter 值加 1 的计数规则，而是采用了一个更优化的计数规则。算法不太复杂，简单来说就是非线性递增的计数器方法，减缓递增速度</p><p>LFU 策略使用衰减因子配置项 <code>lfu_decay_time</code> 来控制访问次数的衰减。LFU 策略会计算当前时间和数据最近一次访问时间的差值，并把这个差值换算成以分钟为单位。然后，LFU 策略再把这个差值除以 <code>lfu_decay_time</code> 值，所得的结果就是数据 counter 要衰减的值。如果 <code>lfu_decay_time</code> 取值更大，那么相应的衰减值会变小，衰减效果也会减弱。所以，如果业务应用中有短时高频访问的数据的话，建议把 <code>lfu_decay_time</code> 值设置为 1，这样一来，LFU 策略在它们不再被访问后，会较快地衰减它们的访问次数，尽早把它们从缓存中淘汰出去，避免缓存污染。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在一些场景下，有些数据被访问的次数非常少，甚至只会被访问一次。当这些数据服务完访问请求后，如果还继续留存在缓存中的话，就只会白白占用缓存空间。这种情况，就是缓存污染。&lt;/p&gt;
&lt;h2 id=&quot;如何解决缓存污染问题？&quot;&gt;&lt;a href=&quot;#如何解决缓存污染问题？&quot; class</summary>
      
    
    
    
    <category term="极客时间Redis实战" scheme="http://universeinheart.github.io/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4Redis%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="Redis" scheme="http://universeinheart.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>26、Redis 缓存异常（下）：如何解决缓存雪崩、击穿、穿透难题？</title>
    <link href="http://universeinheart.github.io/2022/03/16/Redis/26/"/>
    <id>http://universeinheart.github.io/2022/03/16/Redis/26/</id>
    <published>2022-03-15T16:00:00.000Z</published>
    <updated>2022-03-07T11:42:24.036Z</updated>
    
    <content type="html"><![CDATA[<h2 id="缓存雪崩"><a href="#缓存雪崩" class="headerlink" title="缓存雪崩"></a>缓存雪崩</h2><p>缓存雪崩是指大量的应用请求无法在 Redis 缓存中进行处理，紧接着，应用将大量请求发送到数据库层，导致数据库层的压力激增。</p><p><strong>缓存雪崩</strong>，第一个原因是：缓存中有大量数据同时过期，导致大量请求无法得到处理。首先避免给大量的数据设置相同的过期时间，还可以通过<strong>服务降级</strong>，来应对缓存雪崩。</p><img src="/2022/03/16/Redis/26/image-20220307192514484.png" class="" title="image-20220307192514484"><p>除了大量数据同时失效会导致缓存雪崩，还有一种情况也会发生缓存雪崩，那就是，Redis 缓存实例发生故障宕机了，无法处理请求，这就会导致大量请求一下子积压到数据库层，从而发生缓存雪崩。<strong>服务熔断</strong>或是<strong>请求限流机制</strong>，使用这两个机制，来降低雪崩对数据库和整个业务系统的影响。</p><h2 id="缓存击穿"><a href="#缓存击穿" class="headerlink" title="缓存击穿"></a>缓存击穿</h2><p>缓存击穿是指，针对某个访问非常频繁的热点数据的请求，无法在缓存中进行处理，紧接着，访问该数据的大量请求，一下子都发送到了后端数据库，导致了数据库压力激增，会影响数据库处理其他请求。</p><p>缓存击穿的情况，经常发生在<strong>热点数据过期失效</strong>时，为了避免缓存击穿给数据库带来的激增压力，我们的解决方法也比较直接，对于访问特别频繁的热点数据，我们就不设置过期时间了。这样一来，对热点数据的访问请求，都可以在缓存中进行处理，而 Redis 数万级别的高吞吐量可以很好地应对大量的并发请求访问。</p><h2 id="缓存穿透"><a href="#缓存穿透" class="headerlink" title="缓存穿透"></a>缓存穿透</h2><p>缓存穿透是指要访问的数据既不在 Redis 缓存中，也不在数据库中，导致请求在访问缓存时，发生缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据。此时，应用也无法从数据库中读取数据再写入缓存，来服务后续请求，这样一来，缓存也就成了“摆设”，如果应用持续有大量请求访问数据，就会同时给缓存和数据库带来巨大压力</p><p>缓存穿透一般来说，有两种情况。1、<strong>业务层误操作</strong>：缓存中的数据和数据库中的数据被误删除了，2、<strong>恶意攻击</strong>：专门访问数据库中没有的数据。</p><h3 id="缓存穿透应对方案"><a href="#缓存穿透应对方案" class="headerlink" title="缓存穿透应对方案"></a>缓存穿透应对方案</h3><p><strong>1、缓存空值或缺省值。</strong></p><p><strong>2、使用布隆过滤器快速判断数据是否存在，避免从数据库中查询数据是否存在，减轻数据库压力。</strong></p><blockquote><p>布隆过滤器由一个初值都为 0 的 bit 数组和 N 个哈希函数组成，可以用来快速判断某个数据是否存在。</p><p>当我们想标记某个数据存在时（例如，数据已被写入数据库），布隆过滤器会通过三个操作完成标记：</p><ol><li>首先，使用 N 个哈希函数，分别计算这个数据的哈希值，得到 N 个哈希值。</li><li>然后，我们把这 N 个哈希值对 bit 数组的长度取模，得到每个哈希值在数组中的对应位置。</li><li>最后，我们把对应位置的 bit 位设置为 1，这就完成了在布隆过滤器中标记数据的操作。</li></ol><p>如果数据不存在（例如，数据库里没有写入数据），我们也就没有用布隆过滤器标记过数据，那么，bit 数组对应 bit 位的值仍然为 0。</p></blockquote><p>基于布隆过滤器的快速检测特性，我们可以在把数据写入数据库时，使用布隆过滤器做个标记。当缓存缺失后，应用查询数据库时，可以通过查询布隆过滤器快速判断数据是否存在。如果不存在，就不用再去数据库中查询了。这样一来，即使发生缓存穿透了，大量请求只会查询 Redis 和布隆过滤器，而不会积压到数据库，也就不会影响数据库的正常运行。布隆过滤器可以使用 Redis 实现，本身就能承担较大的并发访问压力。</p><p><strong>3、在请求入口的前端进行请求检测</strong></p><p>把恶意的请求（例如请求参数不合理、请求参数是非法值、请求字段不存在）直接过滤掉，不让它们访问后端缓存和数据库。</p><img src="/2022/03/16/Redis/26/image-20220307193904952.png" class="" title="image-20220307193904952">]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;缓存雪崩&quot;&gt;&lt;a href=&quot;#缓存雪崩&quot; class=&quot;headerlink&quot; title=&quot;缓存雪崩&quot;&gt;&lt;/a&gt;缓存雪崩&lt;/h2&gt;&lt;p&gt;缓存雪崩是指大量的应用请求无法在 Redis 缓存中进行处理，紧接着，应用将大量请求发送到数据库层，导致数据库层的压力激增。</summary>
      
    
    
    
    <category term="极客时间Redis实战" scheme="http://universeinheart.github.io/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4Redis%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="Redis" scheme="http://universeinheart.github.io/tags/Redis/"/>
    
  </entry>
  
</feed>
